{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dissertation_models.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "k4fnmQzEztKZ",
        "xt3qvcUpLX2q",
        "5OXE2QPdzb2D",
        "FkPEuEMVzj0i",
        "5XXThA08SV-V",
        "CNW1z59CQalE",
        "eZotr4lVmwsI",
        "nAnpaGnIwwvc",
        "RnOTAdZuLe2T",
        "mLjiSMH9AC5u",
        "h5qpUN9g-aer",
        "7XcFB-7UHtMM",
        "nsDwOA6Qt7M2",
        "f94y3JqcZa_J",
        "FnhdoMSn-dob",
        "P83sR28Uedqj",
        "f247Q2Y3z79T",
        "XVQGP6gwG9vM",
        "ZsDCQk5fJNva",
        "Fh8TZ8zCeQN5",
        "u2Qruftk2RNO",
        "WBMfRgYe-iQg",
        "sJXcCOyvQ3my",
        "m0bc01mJXC-K"
      ],
      "authorship_tag": "ABX9TyPoR+L5QoLXKb6Wz7JEg35c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OscarB1994/Dissertation_project_files-/blob/main/Dissertation_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-nrzhvIjYJU"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvmk9mTqZuMU"
      },
      "source": [
        "%%capture\n",
        "! pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbHsd5ybd70M"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from  torch.utils.data import Dataset\n",
        "\n",
        "import scipy.io as sio\n",
        "import matplotlib.image as image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import cv2\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import datasets, svm, metrics\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix \n",
        "\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import  RandomizedSearchCV, GridSearchCV, cross_val_score, StratifiedKFold, cross_validate, train_test_split\n",
        "\n",
        "from skimage import io, transform\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from skopt.plots import plot_objective, plot_histogram\n",
        "\n",
        "from pylab import imread,subplot,imshow\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "from matplotlib import rc\n",
        "from matplotlib import style \n",
        "import matplotlib.pyplot as plt\n",
        "style.use('seaborn-bright')\n",
        "plt.rcParams['font.family'] = \"serif\"\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmlaf8YXBtXs"
      },
      "source": [
        "def listcheck(list, arg):\n",
        "  '''Checks for particular elements in list and returns total count '''\n",
        "  n = 0\n",
        "  for i in range(len(list)):\n",
        "    if list[i] == arg:\n",
        "      n += 1\n",
        "  return n\n",
        "def unique(list1):\n",
        "  ''''Returns unique elements of set/list'''\n",
        "  list_set = set(list1) \n",
        "  unique_list = (list(list_set)) \n",
        "  return unique_list\n",
        "def get_data(directory, names):\n",
        "    '''Function gather data and assigns corresponding class labels'''\n",
        "    paths, classes = [], []\n",
        "    for i, dir_ in enumerate(names):\n",
        "        for j in os.scandir(directory + dir_):\n",
        "            if (j.is_file()):\n",
        "                paths.append(j.path)\n",
        "                classes.append(i) \n",
        "    return paths, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dddDmB-wLANI"
      },
      "source": [
        "# Gathering the data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILNoXdrontww"
      },
      "source": [
        "label_set = 1\n",
        "param_set = 7 # 7\n",
        "def dataset_dir_selection():\n",
        "  pass\n",
        "lbl = ('Primary_Labels','Secondary_Labels')\n",
        "grp_list = ('Single40th', 'AV10', 'AV20', 'AV30','AV40', 'STD_images', 'ACF1', 'ACF5')\n",
        "directory = f\"/content/gdrive/My Drive/Tomography_Images/{lbl[label_set]}/{grp_list[param_set]}/\"\n",
        "prim_dir = \"/content/gdrive/My Drive/Tomography_Images/Secondary_Unlabelled/\"\n",
        "prim_names = [\"Meas194\",\"Meas196\",\"Meas208\", \"Meas209\", \"Meas214\", \"Meas218\", \"Meas226\",\"Meas227\"]\n",
        "confusion_matrix_title = ['Sing frame', '10 frame average', '20 frame average',\n",
        "                          '30 frame average','40 frame average', '40 frame standard deviation',\n",
        "                          'Auto-correlation: Lag 1', 'Auto-correlation: Lag 5']\n",
        "if label_set ==0:\n",
        "  label, names, name = 'Primary Labels', [\"Bubble_flow\",\"Plug_flow\",\"Stratified_flow\", \"Slug_flow\", \"Annular_flow\"], [\"Bubble\", \"Plug\", \"Stratified\", \"Slug\", \"Annular\"]\n",
        "elif label_set ==1:\n",
        "  label, names, name = 'Secondary Labels', [\"Bubble_flow\",\"Plug_flow\",\"Stratified_flow\",\"Slug_flow\",\"Transient_flow\"], [\"Bubble\", \"Plug\", \"Stratified\", \"Slug\", \"Transient\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnlWc4ai99H-"
      },
      "source": [
        "print('The nominal class tags and class names are:\\n')\n",
        "for i in enumerate(names):\n",
        "  print(i)\n",
        "paths, classes = get_data(directory,\n",
        "                          names)\n",
        "data = {\n",
        "    'path': paths,\n",
        "    'class': classes\n",
        "       }\n",
        "data_df = pd.DataFrame(data,\n",
        "                       columns=['path', 'class'])\n",
        "# shuffling\n",
        "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
        "paths, classes = get_data(directory, names)\n",
        "data = {\n",
        "    'path': paths,\n",
        "    'class': classes\n",
        "       }\n",
        "noshuf_df = pd.DataFrame(data, columns=['path', 'class'])\n",
        "prim_paths, prim_classes = get_data(prim_dir, prim_names)\n",
        "prim_data = {\n",
        "    'path': prim_paths,\n",
        "    'class': prim_classes\n",
        "}\n",
        "prim_df = pd.DataFrame(prim_data, columns=['path', 'class'])\n",
        "prim_df = prim_df.sample(frac=1).reset_index(drop=True)\n",
        "print('Original dataframe')\n",
        "data_df.head()###################################################### used for fitting, validation and testing. \n",
        "class_labels_og = tuple(set(data_df['class'].values.tolist()))\n",
        "print(f'The class labels are {class_labels_og}\\n\\n')\n",
        "print('No shuffle dataframe')\n",
        "noshuf_df.head()\n",
        "class_labels_shuf = tuple(set(noshuf_df['class'].values.tolist()))\n",
        "print(f'The class labels are {class_labels_shuf}\\n\\n')\n",
        "\n",
        "print('Prime dataframe')\n",
        "prim_df.head()\n",
        "class_labels_prime = tuple(set(prim_df['class'].values.tolist()))\n",
        "print(f'The class labels are {class_labels_prime}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4fnmQzEztKZ"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziJ5r9KN0AGe"
      },
      "source": [
        "import copy \n",
        "import timeit\n",
        "\n",
        "print(f\"Found {len(data_df)} images.\")\n",
        "test = data_df.path[1998]\n",
        "x = Image.open(test)\n",
        "img = cv2.imread(test)\n",
        "x\n",
        "freq_2 = [listcheck(data_df['class'], 0),\n",
        "          listcheck(data_df['class'], 1),\n",
        "          listcheck(data_df['class'], 2),\n",
        "          listcheck(data_df['class'], 3),\n",
        "          listcheck(data_df['class'], 4)]\n",
        "          \n",
        "classes_2 = (name)\n",
        "classes2 = np.arange(len(classes_2))\n",
        "plt.bar(classes2, freq_2)\n",
        "plt.title(label)\n",
        "plt.xlabel('Flow Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(classes2, classes_2)\n",
        "plt.show()\n",
        "df = pd.read_csv(\"/content/gdrive/My Drive/Tomography_Images/ERT_files/228-ert.csv\", header = None, skiprows=2).iloc[:, 2:]\n",
        "\n",
        "test = df.iloc[-40::,0].mean()\n",
        "test2 = df.iloc[-20:,0].mean()\n",
        "# groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=<object object>, observed=False, dropna=True\n",
        "print(f'The mean value of the last 40 frames of the first pixel is: {test}')\n",
        "print(f'The mean value of the last 20 frames of the first pixel is: {test2}')\n",
        "print(f'The mean of the two reparameterisations is================: {(test2+test)/2:.2f}')\n",
        "rep_40_df = df.groupby( np.arange(len(df))//40 ).mean()\n",
        "print(f'Before reparameterization: {df.shape[0]} instances.\\n'\\\n",
        "      f'=After 40 frame averaging: {rep_40_df.shape[0]} instances')\n",
        "print('Pixel-wise correlation comparison between reading of both sensors')\n",
        "fig, ax = plt.subplots(1,2)\n",
        "fig.set_figwidth(20)\n",
        "fig.set_figheight(10)\n",
        "img1 = rep_40_df.iloc[:,0:316]\n",
        "ax[0].imshow(img1.corr().fillna(0))\n",
        "img2 = rep_40_df.iloc[:,316:]\n",
        "ax[1].imshow(img2.corr().fillna(0))\n",
        "\n",
        "def Acc_comp():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Why don't we need gradients? What happens if we do include gradients? ---> improves computational speed since grad cal not required for infrence \n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          images, labels = data\n",
        "      \n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model_gpu(images)\n",
        "          \n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          \n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f'Accuracy of the network on the test images: {(100 * correct / total):.2f}%')\n",
        "\n",
        "def Val_Acc_comp():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      \n",
        "      # Iterate over the test set\n",
        "      for data in valid_loader: \n",
        "          images, labels = data\n",
        "          \n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model_gpu(images)\n",
        "          \n",
        "          # torch.max is an argmax operation\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          \n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f'Accuracy of the network on the validation images: { (100 * correct / total):.2f}%')\n",
        "\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm,\n",
        "                          classes,\n",
        "                          normalize=False,\n",
        "                          ticks = True,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Greens):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix very prettily.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, size=18)\n",
        "\n",
        "    # Specify the tick marks and axis text\n",
        "    if ticks:\n",
        "      tick_marks = np.arange(len(classes))\n",
        "      plt.xticks(tick_marks, classes, rotation=90, fontsize=14)\n",
        "      plt.yticks(tick_marks, classes, fontsize=14)\n",
        "\n",
        "    # The data formatting\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    \n",
        "    # Print the text of the matrix, adjusting text colour for display\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                 size=14)\n",
        "\n",
        "    plt.ylabel('True label', fontsize=14)\n",
        "    plt.xlabel('Predicted label', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def validation_stats(valid_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  labeltot =  torch.tensor([], dtype=torch.long)\n",
        "  predtot =  torch.tensor([], dtype=torch.long)\n",
        "  validation_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      \n",
        "      for data in valid_loader:\n",
        "          images, labels = data\n",
        "    \n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model_gpu(images)\n",
        "          \n",
        "          outputted = model_gpu.forward(images)\n",
        "          batch_loss = criterion(outputted, labels)\n",
        "          validation_loss += batch_loss.item()\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          \n",
        "          labeltot = torch.cat((labeltot, labels.cpu()))\n",
        "          predtot = torch.cat((predtot, predicted.cpu()))\n",
        "\n",
        "          \n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  return labeltot, predtot, (100 * correct / total), (validation_loss / len(valid_loader))\n",
        "\n",
        "\n",
        "def test_stats(model_gpu):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  labeltot =  torch.tensor([], dtype=torch.long)\n",
        "  predtot =  torch.tensor([], dtype=torch.long)\n",
        "  test_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      for data in test_loader:\n",
        "          images, labels = data\n",
        "    \n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model_gpu(images)\n",
        "          \n",
        "          outputted = model_gpu.forward(images)\n",
        "          batch_loss = criterion(outputted, labels)\n",
        "          test_loss += batch_loss.item()\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          \n",
        "          labeltot = torch.cat((labeltot, labels.cpu()))\n",
        "          predtot = torch.cat((predtot, predicted.cpu()))\n",
        "\n",
        "          \n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  print(f'Test accuracy: {100*(correct / total):.3f}%')\n",
        "  print(f'Test Loss: {test_loss / len(test_loader):.3f}%')\n",
        "  return labeltot, predtot\n",
        "\n",
        "def validation_stats(valid_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  labeltot =  torch.tensor([], dtype=torch.long)\n",
        "  predtot =  torch.tensor([], dtype=torch.long)\n",
        "  validation_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      \n",
        "      for data in valid_loader:\n",
        "          images, labels = data\n",
        "    \n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model_gpu(images)\n",
        "          \n",
        "          outputted = model_gpu.forward(images)\n",
        "          batch_loss = criterion(outputted, labels)\n",
        "          validation_loss += batch_loss.item()\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          \n",
        "          labeltot = torch.cat((labeltot, labels.cpu()))\n",
        "          predtot = torch.cat((predtot, predicted.cpu()))\n",
        "\n",
        "          \n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  return labeltot, predtot, (100 * correct / total), (validation_loss / len(valid_loader))\n",
        "\n",
        "\n",
        "# to tensor transformation\n",
        "\n",
        "def compute_img_mean_std(image_paths):\n",
        "\n",
        "    imgs = []\n",
        "    means, stdevs = [], []\n",
        "\n",
        "    for i in tqdm(range(len(image_paths))):\n",
        "        img = cv2.imread(image_paths[i])\n",
        "        imgs.append(img)\n",
        "    imgs = np.stack(imgs, axis=2) ####################################################### its not RBG but gray scale\n",
        "    print(f'Shape of processes image tensor {imgs.shape}')\n",
        "    imgs = imgs.astype(np.float32) / 255\n",
        "    # print(imgs)\n",
        "    # for i in range(3):\n",
        "    pixels = imgs[:, :, 0, :].ravel() ###############################################\n",
        "    means.append(np.mean(pixels))\n",
        "    stdevs.append(np.std(pixels))\n",
        "\n",
        "    # means.reverse()  # BGR --> RGB\n",
        "    # stdevs.reverse()\n",
        "    print(f\"normMean = {means}\")\n",
        "    print(f\"normStd = {stdevs}\")\n",
        "    return means, stdevs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDhgIQ3_JIkT"
      },
      "source": [
        "# norm_mean, norm_std = compute_img_mean_std(paths)\n",
        "#### Secondary\n",
        "#av10\n",
        "#norm_mean = [0.5446616] #, 0.5446616, 0.5446616]\n",
        "#norm_std = [0.42959276] #, 0.42959276, 0.42959276]\n",
        "# av20 second\n",
        "#norm_mean = [0.54460377] #, 0.54460377, 0.54460377]\n",
        "#norm_std = [0.42060852] #, 0.42060852, 0.42060852]\n",
        "#av40second\n",
        "norm_mean = [0.54443663]#, 0.54443663, 0.54443663]\n",
        "norm_std = [0.41369697]#, 0.41369697, 0.41369697]\n",
        "#single40th\n",
        "#norm_mean = [0.6505153]# , 0.6505153, 0.6505153]\n",
        "#norm_std = [0.41578674]#, 0.41578674, 0.41578674]\n",
        "#### Primary\n",
        "#AV10\n",
        "#normMean = [0.5497231]# , 0.5497231, 0.5497231]\n",
        "#normStd = [0.42839104]#, 0.42839104, 0.42839104]\n",
        "#AV20\n",
        "#norm_mean = [0.54939455]#, 0.54939455, 0.54939455]\n",
        "#norm_std = [0.4195459]#, 0.4195459, 0.4195459]\n",
        "#acf\n",
        "#normMean = [0.5187708]#, 0.5187708, 0.5187708]\n",
        "#normStd = [0.38183716]#, 0.38183716, 0.38183716]\n",
        "#mean\n",
        "#norm_mean = [0.5495364]#, 0.5495364, 0.5495364] \n",
        "#norm_std = [0.41234276]#, 0.41234276, 0.41234276]\n",
        "#std\n",
        "#norm_mean = [0.30008742]#, 0.30008742, 0.30008742]\n",
        "#norm_std = [0.38573563]# 0.38573563, 0.38573563]\n",
        "#single40th\n",
        "#normMean = [0.6202374]# , 0.6202374, 0.6202374]\n",
        "#normStd = [0.42777553]#, 0.42777553, 0.42777553]\n",
        "\n",
        "#prim_mean, prim_std = compute_img_mean_std(prim_paths)\n",
        "prim_mean = [0.534819] #, 0.534819, 0.534819]\n",
        "prim_std = [0.41308674] #, 0.41308674, 0.41308674]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKot2kCPxy7e"
      },
      "source": [
        "#K Fold CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYV66WpX5_QB"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(20),\n",
        "        transforms.CenterCrop(20),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(norm_mean, norm_std),\n",
        "    ])\n",
        "\n",
        "class Tomography(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        x = Image.open(self.df['path'][index])\n",
        "        y = torch.tensor(int(self.df['class'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y \n",
        "data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKhHXsRYT5ur"
      },
      "source": [
        "train_split = 0.70 \n",
        "valid_split = 0.10\n",
        "batch_size = 32\n",
        "train_size = int(len(data_df)*train_split)\n",
        "valid_size = int(len(data_df)*valid_split)\n",
        "\n",
        "print(f'Number of training instances  : {train_size}')\n",
        "print(f'Number of validating instances: {valid_size}')\n",
        "ins_dataset_train = Tomography(\n",
        "    df=data_df[:(train_size + valid_size)].reset_index(drop=True),\n",
        "    transform=data_transform)\n",
        "ins_dataset_test = Tomography(\n",
        "    df=data_df[(train_size + valid_size):].reset_index(drop=True),\n",
        "    transform=data_transform)\n",
        "instance = 1\n",
        "print(f'Shape of training instance is         : {ins_dataset_train.__getitem__(instance)[0].shape}')\n",
        "print(f'Label of the corresponding instance is: {ins_dataset_train.__getitem__(instance)[1]}')\n",
        "ins_dataset_train.__dict__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35PeTJLlS-_C"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory = True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_test,\n",
        "    batch_size=batch_size, \n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    pin_memory = True)\n",
        "eval_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_test,\n",
        "    batch_size=1, \n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    pin_memory = True)\n",
        "for i, data in enumerate(eval_loader, 0):\n",
        "    images, labels = data\n",
        "    print(f\"Batch {i}\\nSize: {len(images)}\")\n",
        "    break \n",
        "Yt = ins_dataset_test.df['class'].values.tolist()\n",
        "eval_loader.dataset.df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt3qvcUpLX2q"
      },
      "source": [
        "## Architecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2voEeLCMLRW"
      },
      "source": [
        "# Convolutional neural network\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes=len(names)):\n",
        "        super(ConvNet, self).__init__()\n",
        "  \n",
        "        # Add network layers here\n",
        "        self.conv1 = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(in_channels=1,\n",
        "                      out_channels=16,\n",
        "                      kernel_size=(3,3)), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2))#,\n",
        "        self.fc1 = nn.Linear(1296,\n",
        "                             num_classes) \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Complete the graph\n",
        "        out = self.conv1(x)\n",
        "        #out = self.conv2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        #out = self.fc2(out)\n",
        "        return out\n",
        "model = ConvNet()\n",
        "layers = 1\n",
        "runtime = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcHKe7S461Dy"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_gpu = ConvNet().to(device)\n",
        "from torch import optim\n",
        "def weight_reset(model):\n",
        "    for name, module in model.named_children():\n",
        "      if('conv' in name):\n",
        "        torch.nn.init.xavier_uniform_(module[0].weight.data)\n",
        "      else:\n",
        "        module.reset_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OXE2QPdzb2D"
      },
      "source": [
        "##Training K-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_meH7M6vfVu6"
      },
      "source": [
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime \n",
        "\n",
        "def train_kfold_epochs(num_epochs):\n",
        "\n",
        "    global model_gpu\n",
        "    N_splits = 5\n",
        "    total_acc, current_fold = 0, 0\n",
        "    training_losses, validation_losses, validation_accs = [], [], []\n",
        "    best_model_loss, best_acc_fold, best_loss_fold = 10, 10, 10\n",
        "    best_model_accuracy,best_accuracy_epoch, best_loss_epoch  = 0, 0, 0\n",
        "    best_model = []\n",
        "    confusion_matrix_summed = np.zeros((5,5), dtype=int)\n",
        "    kfold = KFold(n_splits= N_splits)\n",
        "####\n",
        "    for fold, (train_index, test_index) in enumerate(kfold.split(ins_dataset_train)):\n",
        "        training_losses = []\n",
        "        validation_losses = []\n",
        "        weight_reset(model_gpu)\n",
        "        train = Subset(ins_dataset_train, train_index)###########################\n",
        "        test = Subset(ins_dataset_train, test_index)\n",
        "        trainloader = DataLoader(train,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0,\n",
        "                                pin_memory=False)#######################\n",
        "        testloader = DataLoader(test,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0,\n",
        "                                pin_memory=False)\n",
        "        for epoch in range(num_epochs):\n",
        "          print(f'\\nFold number {fold + 1} / {kfold.get_n_splits()} \\nEpoch { epoch + 1} / {num_epochs} ')\n",
        "          running_loss = 0.0\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "                images, labels = data\n",
        "                # Explicitly specifies that data is to be copied onto the device!\n",
        "                images = images.to(device)  # <----------- And note it's NOT an in-place operation; original\n",
        "                labels = labels.to(device)  # <----------- variables still exist on CPU\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model_gpu(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                if i % 10 == 9:\n",
        "                  print(f'Epoch / Batch number [{epoch + 1} / {i + 1}]')\n",
        "                  \n",
        "\n",
        "          # Validation data\n",
        "          _, _, validation_accuracy, validation_loss = validation_stats(testloader)\n",
        "          running_loss = running_loss / len(trainloader)\n",
        "          training_losses.append(running_loss)\n",
        "          validation_losses.append(validation_loss) \n",
        "          validation_accs.append(validation_accuracy)  \n",
        "\n",
        "          if best_model_loss > validation_loss:\n",
        "\n",
        "            best_loss_fold = fold + 1\n",
        "            best_model_loss = validation_loss\n",
        "            best_loss_epoch = epoch + 1\n",
        "            best_model = copy.deepcopy(model_gpu)\n",
        "            print('Best model copied')\n",
        "\n",
        "\n",
        "          if best_model_accuracy < validation_accuracy:\n",
        "            best_acc_fold = fold + 1\n",
        "            best_model_accuracy = validation_accuracy\n",
        "            best_accuracy_epoch = epoch + 1\n",
        "\n",
        "          print(\"  >>> \\n\"\\\n",
        "                f\"  Epoch: {epoch+1}/{num_epochs}, Convolutional layers : {layers}\\n\"\\\n",
        "                f\"  Train loss: {running_loss:.3f} \\n\"\\\n",
        "                f\"  Validation loss and Accuracy : {validation_loss:.3f} , {validation_accuracy:.3f}% \\n\"\"  >>> \\n\"\\\n",
        "                f\"  Best Validation Loss and Accuracy: {best_model_loss:.3f} in epoch {best_loss_epoch} and fold {best_loss_fold}, {best_model_accuracy:.3f}% in epoch {best_accuracy_epoch} and fold {best_acc_fold} \\n\")\n",
        "\n",
        "        print(f\"  Best Validation Loss and Accuracy in fold: {best_model_loss:.3f} in epoch {best_loss_epoch} for fold {current_fold}, {best_model_accuracy:.3f}% in epoch {best_accuracy_epoch} for fold {current_fold} \\n\")\n",
        "        current_fold = fold + 1\n",
        "        total_acc += validation_accuracy\n",
        "        plt.plot(training_losses, label='Training loss')\n",
        "        plt.plot(validation_losses, label='Validation loss')\n",
        "        plt.title(f'Fit graph for fold: {current_fold}')\n",
        "        plt.legend(frameon=False)\n",
        "        plt.show()\n",
        "#############################################################################################################\n",
        "        Yp = []\n",
        "        for x,y in eval_loader:\n",
        "          img, label_true = x.to(device), y.to(device)\n",
        "          prediction = model_gpu(img).detach().cpu().numpy().tolist()[0] # using GPU for training routine\n",
        "          y_p = prediction.index(max(prediction))\n",
        "          Yp.append(y_p)\n",
        "\n",
        "        os.chdir('/content/gdrive/My Drive/Tomography_Images/results_info')\n",
        "        time_stamp = str(datetime.now()).replace('-','_').replace(':', '_').replace(' ', '_').split('.')[0] \n",
        "\n",
        "        results = confusion_matrix(Yt,\n",
        "                                   Yp)\n",
        "        confusion_matrix_summed += results\n",
        "        plot_confusion_matrix(results,\n",
        "                              [m.replace('_', ' ') for m in names],\n",
        "                              title =f'Confusion matrix: Fold {fold+1}',\n",
        "                              cmap=plt.cm.Blues)\n",
        "        print(f'\\nThe confusion matrix results for results from fold {fold} are: {results}\\n')\n",
        "        with open(f'confusion_results__fold{fold}_{time_stamp}.pickle', 'wb') as file_handle:\n",
        "          pickle.dump(results, file_handle)\n",
        "    print(f'Average fold accuracy: {total_acc / kfold.get_n_splits():.3f}')\n",
        "\n",
        "    plot_confusion_matrix(confusion_matrix_summed,\n",
        "                          [m.replace('_', ' ') for m in names],\n",
        "                          title =f'Epochs: {runtime}. Total confusion matrix: {confusion_matrix_title[param_set]}')\n",
        "    cls_report = classification_report(Yt,\n",
        "                                       Yp,\n",
        "                                       target_names=name)\n",
        "    \n",
        "    with open(f'classification_report_CNN_{time_stamp}_{lbl[label_set]}_{grp_list[param_set]}.pickle', 'wb') as file_handle:\n",
        "      pickle.dump(cls_report,file_handle)\n",
        "\n",
        "    print(classification_report(Yt,\n",
        "                                Yp,\n",
        "                                target_names=name))\n",
        "    pass\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkPEuEMVzj0i"
      },
      "source": [
        "## Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiNje8msznhK"
      },
      "source": [
        "#train_model_epochs(num_epochs)\n",
        "from torch import optim\n",
        "import timeit\n",
        "runtime = 10\n",
        "LR = 0.0025\n",
        "MO = 0.9\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_gpu = ConvNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Stochastic gradient descent\n",
        "optimizer = optim.SGD(model_gpu.parameters(),\n",
        "                      lr=LR,\n",
        "                      momentum=MO) \n",
        "model_gpu = model_gpu.to(device)\n",
        "cpu_train_time = timeit.timeit(\"train_kfold_epochs(num_epochs)\",\n",
        "                               setup=f\"num_epochs={runtime}\",\n",
        "                               number=1,\n",
        "                               globals=globals())\n",
        "\n",
        "print(f\"learning rate = {LR}\")\n",
        "print(f\"momentum = {MO}\")\n",
        "# test_stats(model_gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6zDauXzbL2s"
      },
      "source": [
        "# CNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XXThA08SV-V"
      },
      "source": [
        "####CNN Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kzW2PeY358r"
      },
      "source": [
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "\n",
        "train_split = 0.70\n",
        "valid_split = 0.10\n",
        "batch_size = 32\n",
        "\n",
        "train_size = int(len(data_df)*train_split) \n",
        "valid_size = int(len(data_df)*valid_split)\n",
        "\n",
        "training = data_df[:train_size]\n",
        "trainval = data_df[:train_size+valid_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkCExF4KkhAU"
      },
      "source": [
        "# **Gaussian noise enrichment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaHXQht3rTwJ"
      },
      "source": [
        "targets = unique(training['class'])\n",
        "size = []\n",
        "extend, boundary = {}, {}\n",
        "for i in targets:\n",
        "  porp = listcheck(training['class'],i)\n",
        "  if i == 0:\n",
        "    lst0 = training.sort_values(\"class\").iloc[0:porp]\n",
        "    l0 = len(lst0)\n",
        "  if i == 1:\n",
        "    lst1 = training.sort_values(\"class\").iloc[l0:l0+porp]\n",
        "    l1 = len(lst1)\n",
        "  if i == 2:\n",
        "    lst2 = training.sort_values(\"class\").iloc[l0+l1:l0+l1+porp]\n",
        "    l2 = len(lst2)\n",
        "  if i == 3:\n",
        "    lst3 = training.sort_values(\"class\").iloc[l0+l1+l2:l0+l1+l2+porp]\n",
        "    l3 = len(lst3)\n",
        "  if i == 4:\n",
        "    lst4 = training.sort_values(\"class\").iloc[l0+l1+l2+l3:l0+l1+l2+l3+porp]\n",
        "\n",
        "    \n",
        "  print(f\"class {i} represents {listcheck(training['class'],i)} of the training elements\")\n",
        "  size.append(listcheck(training['class'],i))\n",
        "\n",
        "for i in targets:\n",
        "  diff = max(size) - listcheck(training['class'],i)\n",
        "  extend[i] = diff\n",
        "  print(f\"{diff} difference for {i}\")\n",
        "  for j in range(diff):\n",
        "    pass\n",
        "\n",
        "# bootstrap sampling for class enrichment ----> i.e. with replacement\n",
        "tba0 = lst0.sample(extend[0], replace = True)\n",
        "tba1 = lst1.sample(extend[1], replace = True)\n",
        "tba2 = lst2.sample(extend[2], replace = True)\n",
        "tba3 = lst3.sample(extend[3], replace = True)\n",
        "tba4 = lst4.sample(extend[4], replace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilaXG2ASRRSX"
      },
      "source": [
        "imag0 = []\n",
        "mean, variance = 0, 0.03 # Gaussian noise parameters  \n",
        "for i in tqdm(range(len(tba0)), desc='Data enrichment for class 0...'):\n",
        "  test = image.imread(tba0.iloc[i][0])\n",
        "  for j in range(20):\n",
        "    for k in range(20):\n",
        "      if test[j][k] == 1:\n",
        "        pass\n",
        "      else:\n",
        "        test[j][k] += abs(test[j][k] + np.random.normal(mean,variance,1))\n",
        "  imag0.append(test)\n",
        "imag1 = []\n",
        "for i in tqdm(range(len(tba1)), desc='Data enrichment for class 1...'):\n",
        "  test = image.imread(tba1.iloc[i][0])\n",
        "  for j in range(20):\n",
        "    for k in range(20):\n",
        "      if test[j][k] == 1:\n",
        "        pass\n",
        "      else:\n",
        "        test[j][k] += abs(test[j][k] + np.random.normal(mean,variance,1))\n",
        "  imag1.append(test)\n",
        "imag2 = []\n",
        "for i in tqdm(range(len(tba2)), desc='Data enrichment for class 2...'):\n",
        "  test = image.imread(tba2.iloc[i][0])\n",
        "  for j in range(20):\n",
        "    for k in range(20):\n",
        "      if test[j][k] == 1:\n",
        "        pass\n",
        "      else:\n",
        "        test[j][k] += abs(test[j][k] + np.random.normal(mean,variance,1))\n",
        "  imag2.append(test)\n",
        "imag3 = []\n",
        "for i in tqdm(range(len(tba3)), desc='Data enrichment for class 3...'):\n",
        "  test = image.imread(tba3.iloc[i][0])\n",
        "  for j in range(20):\n",
        "    for k in range(20):\n",
        "      if test[j][k] == 1:\n",
        "        pass\n",
        "      else:\n",
        "        test[j][k] += abs(test[j][k] + np.random.normal(mean,variance,1))\n",
        "  imag3.append(test)\n",
        "imag4 = []\n",
        "for i in tqdm(range(len(tba4)), desc='Data enrichment for class 4...'):\n",
        "  test = image.imread(tba4.iloc[i][0])\n",
        "  for j in range(20):\n",
        "    for k in range(20):\n",
        "      if test[j][k] == 1:\n",
        "        pass\n",
        "      else:\n",
        "        test[j][k] += abs(test[j][k] + np.random.normal(mean,variance,1))\n",
        "  imag4.append(test)\n",
        "\n",
        "zeros = [0]*len(imag0)\n",
        "ones = [1]*len(imag1)\n",
        "twos = [2]*len(imag2)\n",
        "threes = [3]*len(imag3)\n",
        "fours = [4]*len(imag4)\n",
        "\n",
        "x, y = [], []\n",
        "for i in tqdm(range(train_size), desc=' Loading training data...'):\n",
        "  x.append(np.asarray(Image.open(data_df['path'][i]))/255)\n",
        "  y.append(data_df['class'][i])\n",
        "\n",
        "zip0 = list(zip(imag0,zeros))\n",
        "class_zero = pd.DataFrame(zip0, columns=['path','class'])\n",
        "zip1 = list(zip(imag1,ones))\n",
        "class_one = pd.DataFrame(zip1, columns=['path','class'])\n",
        "zip2 = list(zip(imag2,twos))\n",
        "class_two = pd.DataFrame(zip2, columns=['path','class'])\n",
        "zip3 = list(zip(imag3,threes))\n",
        "class_three = pd.DataFrame(zip3, columns=['path','class'])\n",
        "zip4 = list(zip(imag4,fours))\n",
        "class_four = pd.DataFrame(zip4, columns=['path','class'])\n",
        "\n",
        "zip10 = list(zip(x,y))\n",
        "original_train = pd.DataFrame(zip10, columns=['path','class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPDalZiKolzY"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from  sklearn.model_selection import KFold \n",
        "import random\n",
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(20),\n",
        "        transforms.CenterCrop(20),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(norm_mean, norm_std)])\n",
        "\n",
        "balanced_train = pd.concat([class_zero,\n",
        "                            class_one,\n",
        "                            class_two,\n",
        "                            class_three,\n",
        "                            class_four,\n",
        "                            original_train],\n",
        "                            ignore_index = True)\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax = plt.subplot(111)\n",
        "ax.imshow(balanced_train['path'][5200])\n",
        "X = balanced_train['path'].values.tolist()\n",
        "Y = balanced_train['class'].values.tolist()\n",
        "\n",
        "Xe, Ye, Xt, Yt = X[:int( 0.10 * len(X))], Y[:int( 0.10 * len(X))],  X[int( 0.10 * len(X)):], Y[:int( 0.10 * len(X))]\n",
        "\n",
        "print(f'Total number of examples after enriching data eval: {len(Xe)}')\n",
        "Xe_tensor = torch.tensor(Xe)\n",
        "Ye_tensor = torch.tensor(Ye)\n",
        "\n",
        "Xt_tensor = torch.tensor(Xt)\n",
        "Yt_tensor = torch.tensor(Yt)\n",
        "\n",
        "Xe_added_dim, Xt_added_dim = [], [] \n",
        "for i in range(len(Xe_tensor)):\n",
        "  Xe_added_dim.append(Xe_tensor[i].unsqueeze_(0))\n",
        "  Xt_added_dim.append(Xt_tensor[i].unsqueeze_(0))\n",
        "\n",
        "Xe = torch.stack(Xe_added_dim, dim=0)\n",
        "Xe = Xe.type(torch.cuda.FloatTensor)\n",
        "\n",
        "Xt = torch.stack(Xt_added_dim, dim=0)\n",
        "Xt = Xt.type(torch.cuda.FloatTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHTg_jYc7z6Q"
      },
      "source": [
        "freq_train = [listcheck(balanced_train['class'], 0),\n",
        "              listcheck(balanced_train['class'], 1),\n",
        "              listcheck(balanced_train['class'], 2),\n",
        "              listcheck(balanced_train['class'], 3),\n",
        "              listcheck(balanced_train['class'], 4)]\n",
        "classes_train = (names[0],names[1],names[2],names[3],names[4])\n",
        "classestrain = np.arange(len(classes_train))\n",
        "plt.bar(classestrain, freq_train)\n",
        "plt.title('Primary Labels')\n",
        "plt.xlabel('Flow Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(classestrain, classes_train)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtIaCEDLRT9x"
      },
      "source": [
        "train_split = 0.70 # Defines the ratio of train/valid/test data.\n",
        "valid_split = 0.10\n",
        "batch_size = 32\n",
        "train_size = int(len(Xt)*train_split)\n",
        "valid_size = int(len(Xt)*valid_split)\n",
        "class Tomography_bal_train(Dataset):\n",
        "    def __init__(self,\n",
        "                 df,\n",
        "                 transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        x = Image.fromarray(np.uint8(cm.gray(self.df['path'][index])*255))\n",
        "        y = torch.tensor(int(self.df['class'][index]))\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sLRqU9SqDyJ"
      },
      "source": [
        "## Architecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy2yeIAnqDyN"
      },
      "source": [
        "# Convolutional neural network\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=len(names)):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # Add network layers here\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1,\n",
        "                      out_channels=16,\n",
        "                      kernel_size=(3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc1 = nn.Linear(1296, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "\n",
        "def weight_reset(model):\n",
        "    for name, module in model.named_children():\n",
        "      if('conv' in name):\n",
        "        torch.nn.init.xavier_uniform_(module[0].weight.data)\n",
        "      else:\n",
        "        module.reset_parameters()\n",
        "model_gpu = ConvNet()\n",
        "layers = 1\n",
        "runtime = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNW1z59CQalE"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msQzKbu_Y6JR"
      },
      "source": [
        "def train_cnn_epochs(num_epochs):\n",
        "    \"\"\" Copy of function train_model_epochs but explicitly copying data to device \n",
        "        during training. \n",
        "    \"\"\"\n",
        "    global model_gpu\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    validation_accs = []\n",
        "    best_model_loss = 10\n",
        "    best_model_accuracy = 0\n",
        "    best_model = []\n",
        "    best_accuracy_epoch = 0\n",
        "    best_loss_epoch = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train data\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            images, labels = data\n",
        "            # Explicitly specifies that data is to be copied onto the device!\n",
        "            images = images.to(device)  # <----------- And note it's NOT an in-place operation; original\n",
        "            labels = labels.to(device)  # <----------- variables still exist on CPU\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_gpu(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "              print('Epoch / Batch [%d / %d]' %\n",
        "                      (epoch + 1, i + 1))\n",
        "        # Validation data\n",
        "        _, _, validation_accuracy, validation_loss = validation_stats(valid_loader)\n",
        "        running_loss = running_loss / len(train_loader)\n",
        "        training_losses.append(running_loss)\n",
        "        validation_losses.append(validation_loss) \n",
        "        validation_accs.append(validation_accuracy)  \n",
        "        if best_model_loss > validation_loss:\n",
        "          best_model_loss = validation_loss\n",
        "          best_loss_epoch = epoch + 1\n",
        "          best_model = copy.deepcopy(model_gpu)\n",
        "        if best_model_accuracy < validation_accuracy:\n",
        "          best_model_accuracy = validation_accuracy\n",
        "          best_accuracy_epoch = epoch + 1\n",
        "        print(\"  >>> \\n\"\n",
        "              f\"  Epoch: {epoch+1}/{num_epochs}, Convolutional layers : {layers}\\n\"\n",
        "              f\"  Train loss: {running_loss:.3f} \\n\"\n",
        "              f\"  Validation loss and Accuracy : {validation_loss:.3f} , {validation_accuracy:.3f}% \\n\"\"  >>> \\n\" \n",
        "              f\"  Best Validation Loss and Accuracy: {best_model_loss:.3f} in epoch {best_loss_epoch} , {best_model_accuracy:.3f}% in epoch {best_accuracy_epoch} \\n\"\n",
        "        )\n",
        "        model.train()\n",
        "    plt.plot(training_losses, label='Training loss')\n",
        "    plt.plot(validation_losses, label='Validation loss')\n",
        "    plt.legend(frameon=False)\n",
        "    plt.show()\n",
        "    model_gpu = best_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buk1N3GgRQrr"
      },
      "source": [
        "##Training K-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTfXzn2iRQrx"
      },
      "source": [
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime \n",
        "def train_kfold_epochs(num_epochs):\n",
        "    global model_gpu\n",
        "    N_splits = 5\n",
        "    total_acc, current_fold = 0, 0\n",
        "    training_losses, validation_losses, validation_accs, best_model = [], [], [], []\n",
        "    best_model_loss, best_acc_fold, best_loss_fold = 10, 10, 10\n",
        "    best_model_accuracy,best_accuracy_epoch, best_loss_epoch  = 0, 0, 0\n",
        "    confusion_matrix_summed = np.zeros((5,5),\n",
        "                                       dtype=int)\n",
        "    kfold = KFold(n_splits= N_splits)\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(kfold.split(list(zip(X,Y_tensor)))):\n",
        "        training_losses = []\n",
        "        validation_losses = []\n",
        "        weight_reset(model_gpu)\n",
        "\n",
        "        data_train = TensorDataset(Xt[train_index], Yt_tensor[train_index])\n",
        "        data_test = TensorDataset(Xt[test_index], Yt_tensor[test_index])\n",
        "        data_eval = TensorDataset(Xe, Ye_tensor)\n",
        "\n",
        "        train_loader = DataLoader(data_train,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True)\n",
        "        train_loader = DataLoader(data_test,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True)\n",
        "      \n",
        "        eval_loader = DataLoader(data_eval, \n",
        "                                 batch_size=1)\n",
        "        # trainloader = DataLoader(train,\n",
        "        #                         batch_size=batch_size,\n",
        "        #                         shuffle=True,\n",
        "        #                         num_workers=0,\n",
        "        #                         pin_memory=False)#######################\n",
        "        # testloader = DataLoader(test,\n",
        "        #                         batch_size=batch_size,\n",
        "        #                         shuffle=True,\n",
        "        #                         num_workers=0,\n",
        "        #                         pin_memory=False)\n",
        "        for epoch in range(num_epochs):\n",
        "          print(f'\\nFold number {fold + 1} / {kfold.get_n_splits()} \\nEpoch { epoch + 1} / {num_epochs} ')\n",
        "          running_loss = 0.0\n",
        "          for i, data in enumerate(train_loader, 0):\n",
        "                images, labels = data\n",
        "                # Explicitly specifies that data is to be copied onto the device!\n",
        "                images = images.to(device)  # <----------- And note it's NOT an in-place operation; original\n",
        "                labels = labels.to(device)  # <----------- variables still exist on CPU\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model_gpu(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                if i % 10 == 9:\n",
        "                  print(f'Epoch / Batch number [{epoch + 1} / {i + 1}]')\n",
        "\n",
        "          # Validation data\n",
        "          _, _, validation_accuracy, validation_loss = validation_stats(test_loader)\n",
        "          running_loss = running_loss / len(train_loader)\n",
        "          training_losses.append(running_loss)\n",
        "          validation_losses.append(validation_loss) \n",
        "          validation_accs.append(validation_accuracy)  \n",
        "          if best_model_loss > validation_loss:\n",
        "            best_loss_fold = fold + 1\n",
        "            best_model_loss = validation_loss\n",
        "            best_loss_epoch = epoch + 1\n",
        "            best_model = copy.deepcopy(model_gpu)\n",
        "            print('Best model copied')\n",
        "          if best_model_accuracy < validation_accuracy:\n",
        "            best_acc_fold = fold + 1\n",
        "            best_model_accuracy = validation_accuracy\n",
        "            best_accuracy_epoch = epoch + 1\n",
        "          print(\"  >>> \\n\"\\\n",
        "                f\"  Epoch: {epoch+1}/{num_epochs}, Convolutional layers : {layers}\\n\"\\\n",
        "                f\"  Train loss: {running_loss:.3f} \\n\"\\\n",
        "                f\"  Validation loss and Accuracy : {validation_loss:.3f} , {validation_accuracy:.3f}% \\n\"\"  >>> \\n\"\\\n",
        "                f\"  Best Validation Loss and Accuracy: {best_model_loss:.3f} in epoch {best_loss_epoch} and fold {best_loss_fold}, {best_model_accuracy:.3f}% in epoch {best_accuracy_epoch} and fold {best_acc_fold} \\n\")\n",
        "\n",
        "        print(f\"  Best Validation Loss and Accuracy in fold: {best_model_loss:.3f} in epoch {best_loss_epoch} for fold {current_fold}, {best_model_accuracy:.3f}% in epoch {best_accuracy_epoch} for fold {current_fold} \\n\")\n",
        "        current_fold = fold + 1\n",
        "        total_acc += validation_accuracy\n",
        "        plt.plot(training_losses, label='Training loss')\n",
        "        plt.plot(validation_losses, label='Validation loss')\n",
        "        plt.title(f'Fit graph for fold: {current_fold}')\n",
        "        plt.legend(frameon=False)\n",
        "        plt.show()\n",
        "#############################################################################################################\n",
        "        Yp = []\n",
        "        for x,y in eval_loader:\n",
        "          img, label_true = x.to(device), y.to(device)\n",
        "          prediction = model_gpu(img).detach().cpu().numpy().tolist()[0] # using GPU for training routine\n",
        "          y_p = prediction.index(max(prediction))\n",
        "          Yp.append(y_p)\n",
        "\n",
        "        os.chdir('/content/gdrive/My Drive/Tomography_Images/results_info')\n",
        "        time_stamp = str(datetime.now()).replace('-','_').replace(':', '_').replace(' ', '_').split('.')[0] \n",
        "\n",
        "        results = confusion_matrix(Yt,\n",
        "                                   Yp)\n",
        "        confusion_matrix_summed += results\n",
        "        plot_confusion_matrix(results,\n",
        "                              [m.replace('_', ' ') for m in names],\n",
        "                              title =f'Confusion matrix: Fold {fold+1}',\n",
        "                              cmap=plt.cm.Blues)\n",
        "        print(f'\\nThe confusion matrix results for results from fold {fold} are: {results}\\n')\n",
        "        with open(f'confusion_results__fold{fold}_{time_stamp}.pickle', 'wb') as file_handle:\n",
        "          pickle.dump(results, file_handle)\n",
        "    print(f'Average fold accuracy: {total_acc / kfold.get_n_splits():.3f}')\n",
        "    plot_confusion_matrix(confusion_matrix_summed,\n",
        "                          [m.replace('_', ' ') for m in names],\n",
        "                          title =f'Epochs: {runtime}. Total confusion matrix: {confusion_matrix_title[param_set]}')\n",
        "    cls_report = classification_report(Yt,\n",
        "                                       Yp,\n",
        "                                       target_names=name)\n",
        "    with open(f'classification_report_CNN_{time_stamp}_{lbl[label_set]}_{grp_list[param_set]}.pickle', 'wb') as file_handle:\n",
        "      pickle.dump(cls_report,file_handle)\n",
        "    print(classification_report(Yt,\n",
        "                                Yp,\n",
        "                                target_names=name))\n",
        "    pass\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiZlvovf794e"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMqUWjan76xU"
      },
      "source": [
        "from torch import optim\n",
        "LR, MO, runtime = 0.0025, 0.9, 10\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_gpu = ConvNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model_gpu.parameters(),\n",
        "                      lr=LR,\n",
        "                      momentum=MO)\n",
        " \n",
        "model_gpu = model_gpu.to(device)\n",
        "\n",
        "cpu_train_time = timeit.timeit(\n",
        "    \"train_kfold_epochs(num_epochs)\",\n",
        "    setup=f\"num_epochs={runtime}\",\n",
        "    number=1,\n",
        "    globals=globals())\n",
        "\n",
        "print(f\"learning rate = {LR}\")\n",
        "print(f\"momentum = {MO}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZotr4lVmwsI"
      },
      "source": [
        "## Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_fj3nqqPDRB"
      },
      "source": [
        "ins_dataset_prim = Tomography(\n",
        "    df= prim_data,\n",
        "    transform= transforms.Compose([\n",
        "        transforms.Resize(20),\n",
        "        transforms.CenterCrop(20),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(prim_mean, prim_std),\n",
        "    ]),\n",
        ")\n",
        "\n",
        "primlab_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_prim,\n",
        "    batch_size=batch_size, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True)\n",
        "\n",
        "counter = []\n",
        "\n",
        "for numb in range(len(prim_df['path'])):\n",
        "  image, label = primlab_loader.dataset[numb]\n",
        "  image = image.type(torch.cuda.FloatTensor)\n",
        "  classe = model_gpu.forward(image.unsqueeze(0))\n",
        "  _ , predicted = torch.max(classe, 1)\n",
        "  counter.append(predicted.item())\n",
        "  print(f'Label: {label} , Class: {predicted.item()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqpqamMJEidf"
      },
      "source": [
        "print(f'The number of predictions is:{len(counter)}')\n",
        "prim_names = [\n",
        "  \"Meas194\",\n",
        "  \"Meas196\",\n",
        "  \"Meas208\",\n",
        "  \"Meas209\",\n",
        "  \"Meas214\",\n",
        "  \"Meas218\",\n",
        "  \"Meas226\",\n",
        "  \"Meas227\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRI0VBky9v9l"
      },
      "source": [
        "meas_194 = counter[0:250]\n",
        "meas_196 = counter[250:500]\n",
        "meas_208 = counter[500:750]\n",
        "meas_209 = counter[750:1000]\n",
        "meas_214 = counter[1000:1250]\n",
        "meas_218 = counter[1250:1500]\n",
        "meas_226 = counter[1500:1750]\n",
        "meas_227 = counter[1750:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHrwpCnkaaMX"
      },
      "source": [
        "plt.hist(meas_194, bins = [0, 1, 2, 3, 4, 5], rwidth = 0.9, align = 'left');\n",
        "plt.title('Meas_194')\n",
        "plt.xlabel('Class Prediction');\n",
        "plt.ylabel('Count');\n",
        "plt.xticks(np.arange(5), ('Bubble', 'Plug', 'Stratified', 'Slug', 'Transient'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S35I9d9p-aPI"
      },
      "source": [
        "plt.hist(meas_196, bins = [0, 1, 2, 3, 4, 5], rwidth = 0.9, align = 'left');\n",
        "plt.title('Meas_196')\n",
        "plt.xlabel('Class Prediction');\n",
        "plt.ylabel('Count');\n",
        "plt.xticks(np.arange(5), ('Bubble', 'Plug', 'Stratified', 'Slug', 'Transient'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z93eXf05GfnO"
      },
      "source": [
        "plt.hist(meas_208, bins = [0, 1, 2, 3, 4, 5], rwidth = 0.9, align = 'left');\n",
        "plt.title('Meas_208')\n",
        "plt.xlabel('Class Prediction');\n",
        "plt.ylabel('Count');\n",
        "plt.xticks(np.arange(5), ('Bubble', 'Plug', 'Stratified', 'Slug', 'Transient'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDgWaS4uaele"
      },
      "source": [
        "plt.hist(meas_209, bins = [0, 1, 2, 3, 4, 5], rwidth = 0.9, align = 'left');\n",
        "plt.title('Meas_209')\n",
        "plt.xlabel('Class Prediction');\n",
        "plt.ylabel('Count');\n",
        "plt.xticks(np.arange(5), ('Bubble', 'Plug', 'Stratified', 'Slug', 'Transient'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP_dXdbdGjYI"
      },
      "source": [
        "plt.hist(meas_214, bins = [0, 1, 2, 3, 4, 5], rwidth = 0.9, align = 'left');\n",
        "plt.title('Meas_214')\n",
        "plt.xlabel('Class Prediction');\n",
        "plt.ylabel('Count');\n",
        "plt.xticks(np.arange(5), ('Bubble', 'Plug', 'Stratified', 'Slug', 'Transient'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKZLggW_GkNL"
      },
      "source": [
        "plt.hist(meas_218, bins = [0, 1, 2, 3, 4, 5], rwidth = 0.9, align = 'left');\n",
        "plt.title('Meas_218')\n",
        "plt.xlabel('Class Prediction');\n",
        "plt.ylabel('Count');\n",
        "plt.xticks(np.arange(5), ('Bubble', 'Plug', 'Stratified', 'Slug', 'Transient'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "camt4R9EGnQ9"
      },
      "source": [
        "plt.hist(meas_226, bins = [0, 1, 2, 3, 4, 5], rwidth = 0.9, align = 'left');\n",
        "plt.title('Meas_226')\n",
        "plt.xlabel('Class Prediction');\n",
        "plt.ylabel('Count');\n",
        "plt.xticks(np.arange(5), ('Bubble', 'Plug', 'Stratified', 'Slug', 'Transient'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evhwala7GnnD"
      },
      "source": [
        "plt.hist(meas_227, bins = [0, 1, 2, 3, 4, 5], rwidth = 0.9, align = 'left');\n",
        "plt.title('Meas_227')\n",
        "plt.xlabel('Class Prediction');\n",
        "plt.ylabel('Count');\n",
        "plt.xticks(np.arange(5), ('Bubble', 'Plug', 'Stratified', 'Slug', 'Transient'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZdo60evIyu_"
      },
      "source": [
        "# Save model\n",
        "#path = f\"/content/gdrive/My Drive/Models/{layers}_Layers_40avgims.py\"\n",
        "#torch.save(model_gpu, F\"{path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqgRjVOfzd0U"
      },
      "source": [
        "### Vis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-npVxexOlpz"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "PCA_List = []\n",
        "for i in range(len(data_df)):\n",
        "  PCA_List.append(np.array(Image.open(data_df.path[i])))\n",
        "  if i%100 == 0:\n",
        "    print(f\"{(i/len(data_df)*100):.2f}% complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG3V-5LBrDTc"
      },
      "source": [
        "x = len(PCA_List)\n",
        "y = len(PCA_List[1].flatten())\n",
        "z = np.zeros((x,y))\n",
        "\n",
        "for i in range(x):\n",
        "  z[i] = PCA_List[i].flatten()\n",
        "pca = PCA(10)\n",
        "z_norm = normalize(z, norm='l1', axis=1, copy=True, return_norm=False)\n",
        "lower_dimension_data = pca.fit_transform(z_norm)\n",
        "approx = pca.inverse_transform(lower_dimension_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDdRxgVskMo6"
      },
      "source": [
        "pca.components_[1]\n",
        "loading_scores = pd.Series(pca.components_[1])\n",
        "loading_scores.plot.bar(y='Loading Score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR6EORuPXYAr"
      },
      "source": [
        "plt.scatter(lower_dimension_data[:, 0], lower_dimension_data[:, 1],\n",
        "            c=data_df[\"class\"],edgecolor='none', alpha=0.75, cmap=plt.cm.get_cmap('Set3', 5))\n",
        "\n",
        "plt.title(\"Variance Explained: 0.874\")\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')\n",
        "cbar = plt.colorbar(ticks = [0,1,2,3,4])\n",
        "cbar.ax.set_yticklabels(['Bubble', 'Plug','Stratified','Slug', 'Annular'])\n",
        "# CLASSES ARE AS FOLLOWS:\n",
        "# Bubble = 0\n",
        "# Annular = 1\n",
        "# Plug = 2\n",
        "# Stratified = 3\n",
        "# Slug = 4\n",
        "#or\n",
        "#  \"Bubble_Flow_Wang\",\n",
        "#  \"Plug_Flow\",\n",
        "#  \"Stratified_Flow_balancedgroup\",\n",
        "#  \"Slug_Flow_Wang\",\n",
        "#  \"Transient_Flow_balancedgroup\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8CYkzivo7AF"
      },
      "source": [
        "plt.scatter(lower_dimension_data[:, 0],\n",
        "            lower_dimension_data[:, 2],\n",
        "            c=data_df[\"class\"],\n",
        "            edgecolor='none',\n",
        "            alpha=0.75,\n",
        "            cmap=plt.cm.get_cmap('Set3', 5))\n",
        "\n",
        "\n",
        "# zip joins x and y coordinates in pairs\n",
        "#for x,y in zip(lower_dimension_data[:, 0], lower_dimension_data[:, 1]):\n",
        "#    label = \"{:1f}\".format()\n",
        "#    plt.annotate(label, # this is the text\n",
        "#                 (x,y), # this is the point to label\n",
        "#                 textcoords=\"offset points\", # how to position the text\n",
        "#                 xytext=(0,10), # distance from text to points (x,y)\n",
        "#                 ha='center') # horizontal alignment can be left, right or center\n",
        "plt.title(\"Variance Explained: 0.802\")\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 3')\n",
        "cbar = plt.colorbar(ticks = [0,1,2,3,4])\n",
        "cbar.ax.set_yticklabels(['Bubble', 'Plug','Stratified','Slug', 'Annular'])\n",
        "\n",
        "# CLASSES ARE AS FOLLOWS:\n",
        "# Bubble = 0\n",
        "# Annular = 1\n",
        "# Plug = 2\n",
        "# Stratified = 3\n",
        "# Slug = 4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH2WzCUzpDny"
      },
      "source": [
        "plt.scatter(lower_dimension_data[:, 0], lower_dimension_data[:, 3],\n",
        "            c=data_df[\"class\"],edgecolor='none', alpha=0.75, cmap=plt.cm.get_cmap('Set3', 5))\n",
        "# zip joins x and y coordinates in pairs\n",
        "#for x,y in zip(lower_dimension_data[:, 0], lower_dimension_data[:, 1]):\n",
        "#    label = \"{:1f}\".format()\n",
        "#    plt.annotate(label, # this is the text\n",
        "#                 (x,y), # this is the point to label\n",
        "#                 textcoords=\"offset points\", # how to position the text\n",
        "#                 xytext=(0,10), # distance from text to points (x,y)\n",
        "#                 ha='center') # horizontal alignment can be left, right or center\n",
        "plt.title(\"Variance Explained: 0.776\")\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 4')\n",
        "cbar = plt.colorbar(ticks = [0,1,2,3,4])\n",
        "cbar.ax.set_yticklabels(['Bubble', 'Plug','Stratified','Slug', 'Annular'])\n",
        "\n",
        "# CLASSES ARE AS FOLLOWS:\n",
        "# Bubble = 0\n",
        "# Annular = 1\n",
        "# Plug = 2\n",
        "# Stratified = 3\n",
        "# Slug = 4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTHazh99p-zg"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x =lower_dimension_data[:, 0]\n",
        "y =lower_dimension_data[:, 1]\n",
        "z =lower_dimension_data[:, 2]\n",
        "\n",
        "ax.scatter (x, y,  z,\n",
        "            marker='o',\n",
        "            c=data_df[\"class\"],\n",
        "            alpha=0.75,\n",
        "            cmap=plt.cm.get_cmap('Set3', 5))\n",
        "\n",
        "ax.set_xlabel('component 1')\n",
        "ax.set_ylabel('component 2')\n",
        "ax.set_zlabel('component 3')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UCh1Ni9QoXM"
      },
      "source": [
        "pca.components_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEMlgDZGB6cP"
      },
      "source": [
        "imshow(PCA_List[2].reshape(20,20), cmap = \"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRWralrrCG04"
      },
      "source": [
        "imshow(approx[2].reshape(20,20), cmap = \"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMCXaDw2e1Mm"
      },
      "source": [
        "Image.open(data_df.path[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OcgxeyhP-r9"
      },
      "source": [
        "print(\"znorm\",z_norm.shape)\n",
        "print(\"Components Shape\",pca.components_.shape)\n",
        "print(\"PCA Components\",pca.components_)\n",
        "print(\"Variance Shape\",pca.explained_variance_.shape)\n",
        "print(\"PCA Variances\",pca.explained_variance_)\n",
        "\n",
        "print(\"znorm Shape\",z_norm.shape)\n",
        "print(\"Lower dim Shape\",lower_dimension_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0nQ7mYJ816I"
      },
      "source": [
        "v = pca.explained_variance_ratio_.cumsum()\n",
        "v2 = pca.explained_variance_ratio_\n",
        "v[:]\n",
        "plt.bar(x = range(1,len(v2)+1), height = v2, tick_label = range(1,len(v2)+1))\n",
        "plt.ylabel('Percentage of variance explained')\n",
        "plt.xlabel('Principal component')\n",
        "plt.title('Secondary labels averaged every 40th frame')\n",
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbYwHOsYV5XD"
      },
      "source": [
        "pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[3] \n",
        "approx[1].reshape(20,20).shape\n",
        "PCA_List[1].shape\n",
        "PCAed_List = []\n",
        "\n",
        "for i in range(len(approx)):\n",
        "  PCAed_List.append(approx[i].reshape(20,20))\n",
        "for i in range(len(PCAed_List)):\n",
        "  PCAed_List[i] = torch.from_numpy(PCAed_List[i]).unsqueeze(0).type(torch.FloatTensor)\n",
        "data_df[\"PCA_ims\"] = PCAed_List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODPMMHPechH7"
      },
      "source": [
        "### PCA application\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIA9kYb7NFFQ"
      },
      "source": [
        "class Tomography(Dataset):\n",
        "\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = Image.open(self.df['path'][index])\n",
        "        y = torch.tensor(int(self.df['class'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y \n",
        "#Class for PCAed images only\n",
        "class Tomography_pca(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        z = self.df['PCA_ims'][index]\n",
        "        y = torch.tensor(int(self.df['class'][index]))\n",
        "        return z, y #,z/x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNjQpmk-fwDi"
      },
      "source": [
        "train_split = 0.70 # Defines the ratio of train/valid/test data.\n",
        "valid_split = 0.10\n",
        "batch_size = 32\n",
        "\n",
        "train_size = int(len(data_df)*train_split)\n",
        "valid_size = int(len(data_df)*valid_split)\n",
        "\n",
        "ins_dataset_train = Tomography_pca(\n",
        "    df=data_df[:train_size],\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "ins_dataset_valid = Tomography_pca(\n",
        "    df=data_df[train_size:(train_size + valid_size)].reset_index(drop=True),\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "ins_dataset_test = Tomography_pca(\n",
        "    df=data_df[(train_size + valid_size):].reset_index(drop=True),\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_train,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory = True\n",
        "\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_valid,\n",
        "    batch_size=batch_size, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory = True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_test,\n",
        "    batch_size=batch_size, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAnpaGnIwwvc"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfI8IFIjfqxL"
      },
      "source": [
        "# Convolutional neural network\n",
        "names = [0,1,2,3,4]\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes=len(names)):\n",
        "        super(ConvNet, self).__init__()\n",
        "  \n",
        "        # Add network layers here\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            #,nn.Dropout(p = 0.3))\n",
        "        #self.conv2 = nn.Sequential(\n",
        "            #nn.Conv2d(16, 24, 4),\n",
        "            #nn.ReLU(),\n",
        "            #nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #nn.Dropout(p = 0.3))\n",
        "        #self.fc1 = nn.Linear(216, 512) \n",
        "        #self.fc2 = nn.Linear(512, num_classes)\n",
        "        ## With just one Conv\n",
        "        self.fc1 = nn.Linear(1296, num_classes) \n",
        "        #self.fc2 = nn.Linear(216, num_classes) \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Complete the graph\n",
        "        out = self.conv1(x)\n",
        "        #out = self.conv2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        \n",
        "        out = self.fc1(out)\n",
        "        #out = self.fc2(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "model = ConvNet()\n",
        "layers = 2\n",
        "runtime = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLDsUiN_feQw"
      },
      "source": [
        "#train_model_epochs(num_epochs)\n",
        "from torch import optim\n",
        "LR = 0.025\n",
        "MO = 0.9\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_gpu = ConvNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Stochastic gradient descent\n",
        "optimizer = optim.SGD(model_gpu.parameters(), lr=LR, momentum=MO) \n",
        "\n",
        "model_gpu = model_gpu.to(device)\n",
        "cpu_train_time = timeit.timeit(\n",
        "    \"train_cnn_epochs(num_epochs)\",\n",
        "    setup=f\"num_epochs={runtime}\",\n",
        "    number=1,\n",
        "    globals=globals(),\n",
        ")\n",
        "\n",
        "print(f\"learning rate = {LR}\")\n",
        "print(f\"momentum = {MO}\")\n",
        "test_stats(model_gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2uUndhFvY0f"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labeltot, predtot, _, _ = test_stats(model_gpu)\n",
        "cm = confusion_matrix(labeltot, predtot)\n",
        "plot_confusion_matrix(cm, names)\n",
        "\n",
        "print(\">>> \\n\"\n",
        "              \"Test Accuracy: \\n\"\n",
        "              f\"{(100*sum(cm.diagonal())/sum(sum(cm[:,:]))):.2f}% \\n\"\n",
        "              )\n",
        "test_stats(model_gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWaFaH4VBIOK"
      },
      "source": [
        "### PCA dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TQPGRQvJxqz"
      },
      "source": [
        "len(noshuf_df['class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFLXeDiAzr5c"
      },
      "source": [
        "bublist = []\n",
        "pluglist = []\n",
        "stratlist = []\n",
        "sluglist = []\n",
        "tranlist = []\n",
        "for i in range(len(noshuf_df)):\n",
        "  if noshuf_df['class'][i] == 0:\n",
        "    bublist.append(np.array(Image.open(noshuf_df.path[i])).flatten())\n",
        "  elif noshuf_df['class'][i] == 1:\n",
        "    pluglist.append(np.array(Image.open(noshuf_df.path[i])).flatten())\n",
        "  elif noshuf_df['class'][i] == 2:\n",
        "    stratlist.append(np.array(Image.open(noshuf_df.path[i])).flatten())\n",
        "  elif noshuf_df['class'][i] == 3:\n",
        "    sluglist.append(np.array(Image.open(noshuf_df.path[i])).flatten())\n",
        "  elif noshuf_df['class'][i] == 4:\n",
        "    tranlist.append(np.array(Image.open(noshuf_df.path[i])).flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLmJTxYEUg-j"
      },
      "source": [
        "len(tranlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GffB5NElDwPQ"
      },
      "source": [
        "imshow(bublist[20].reshape(20,20), cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVIq1d_dzr5g"
      },
      "source": [
        "pca1 = PCA(2)\n",
        "pca2 = PCA(2)\n",
        "pca3 = PCA(2)\n",
        "pca4 = PCA(2)\n",
        "pca5 = PCA(2)\n",
        "bub_norm = normalize(bublist, norm='l1', axis=1, copy=True, return_norm=False)\n",
        "plug_norm = normalize(pluglist, norm='l1', axis=1, copy=True, return_norm=False)\n",
        "strat_norm = normalize(stratlist, norm='l1', axis=1, copy=True, return_norm=False)\n",
        "slug_norm = normalize(sluglist, norm='l1', axis=1, copy=True, return_norm=False)\n",
        "trans_norm = normalize(tranlist, norm='l1', axis=1, copy=True, return_norm=False)\n",
        "\n",
        "lower_dimension_bub = pca1.fit_transform(bub_norm)\n",
        "bub_approx = pca1.inverse_transform(lower_dimension_bub)\n",
        "lower_dimension_plug = pca2.fit_transform(plug_norm)\n",
        "plug_approx = pca2.inverse_transform(lower_dimension_plug)\n",
        "lower_dimension_strat = pca3.fit_transform(strat_norm)\n",
        "strat_approx = pca3.inverse_transform(lower_dimension_strat)\n",
        "lower_dimension_slug = pca4.fit_transform(slug_norm)\n",
        "slug_approx = pca4.inverse_transform(lower_dimension_slug)\n",
        "lower_dimension_trans = pca5.fit_transform(trans_norm)\n",
        "trans_approx = pca5.inverse_transform(lower_dimension_trans)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QxGxX8oU0Zp"
      },
      "source": [
        "pca2.components_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WttMXAS4Vk3X"
      },
      "source": [
        "print(pca1.explained_variance_ratio_.cumsum(),\n",
        "      pca2.explained_variance_ratio_.cumsum(),\n",
        "      pca3.explained_variance_ratio_.cumsum(),\n",
        "      pca4.explained_variance_ratio_.cumsum(),\n",
        "      pca5.explained_variance_ratio_.cumsum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfUlfAjNEdJ-"
      },
      "source": [
        "plt.scatter(lower_dimension_bub[:, 0], lower_dimension_bub[:, 1],\n",
        "          edgecolor='none', alpha=0.75, cmap=plt.cm.get_cmap('Set3', 5))\n",
        "plt.title(\"Bubble Flow (secondary): 0.994 variance\")\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAd-SjxP9_HV"
      },
      "source": [
        "plt.scatter(lower_dimension_plug[:, 0], lower_dimension_plug[:, 1],\n",
        "          edgecolor='none', alpha=0.75, cmap=plt.cm.get_cmap('Set3', 5))\n",
        "plt.title(\"Plug Flow: 0.753 variance\")\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T_SMbxl-NnJ"
      },
      "source": [
        "plt.scatter(lower_dimension_strat[:, 0], lower_dimension_strat[:, 1],\n",
        "          edgecolor='none', alpha=0.75, cmap=plt.cm.get_cmap('Set3', 5))\n",
        "plt.title(\"Stratified Flow: 0.716 variance\")\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LhcXWKl-RNp"
      },
      "source": [
        "plt.scatter(lower_dimension_slug[:, 0], lower_dimension_slug[:, 1],\n",
        "          edgecolor='none', alpha=0.75, cmap=plt.cm.get_cmap('Set3', 5))\n",
        "plt.title(\"Slug Flow (secondary): 0.808 variance\")\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlyPcFFM-WtE"
      },
      "source": [
        "plt.scatter(lower_dimension_trans[:, 0], lower_dimension_trans[:, 1],\n",
        "          edgecolor='none', alpha=0.75, cmap=plt.cm.get_cmap('Set3', 5))\n",
        "plt.title(\"Transient Flow: 0.767 variance\")\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')\n",
        "\n",
        "bub_approx.shape\n",
        "imshow(stratlist[12].reshape(20,20), cmap = 'gray')\n",
        "imshow(strat_approx[12].reshape(20,20), cmap = 'gray')\n",
        "lst = []\n",
        "for i in range(len(bub_approx)):\n",
        "  lst.append(bub_approx[i].reshape(20,20))\n",
        "for i in range(len(plug_approx)):\n",
        "  lst.append(plug_approx[i].reshape(20,20))\n",
        "for i in range(len(strat_approx)):\n",
        "  lst.append(strat_approx[i].reshape(20,20))\n",
        "for i in range(len(slug_approx)):\n",
        "  lst.append(slug_approx[i].reshape(20,20))\n",
        "for i in range(len(trans_approx)):\n",
        "  lst.append(trans_approx[i].reshape(20,20))\n",
        "for i in range(len(lst)):\n",
        "  lst[i] = torch.from_numpy(lst[i]).unsqueeze(0).type(torch.FloatTensor)\n",
        "len(lst)\n",
        "noshuf_df\n",
        "noshuf_df[\"PCA_ims\"] = lst\n",
        "noshuf_df = noshuf_df.sample(frac=1).reset_index(drop=True) # Shuffles the data\n",
        "noshuf_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8s8OOGj0m8-"
      },
      "source": [
        "#Class for PCAed images only\n",
        "\n",
        "class Tomography(Dataset):\n",
        "    \"\"\" Tomography dataset. \"\"\"\n",
        "\n",
        "    def __init__(self, df, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (string): Directory with all the images\n",
        "            df (DataFrame object): Dataframe containing the images, paths and classes\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load image from path and get label\n",
        "        z = self.df['PCA_ims'][index]\n",
        "        y = torch.tensor(int(self.df['class'][index]))\n",
        "\n",
        "        return z, y #,z/x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyjFnOo7xEmE"
      },
      "source": [
        "imshow(lst[597].reshape(20,20), cmap = \"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAVaKGrABBt2"
      },
      "source": [
        "train_split = 0.70 # Defines the ratio of train/valid/test data.\n",
        "valid_split = 0.10\n",
        "batch_size = 32\n",
        "\n",
        "train_size = int(len(noshuf_df)*train_split)\n",
        "valid_size = int(len(noshuf_df)*valid_split)\n",
        "\n",
        "ins_dataset_train = Tomography(\n",
        "    df=noshuf_df[:train_size],\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "ins_dataset_valid = Tomography(\n",
        "    df=noshuf_df[train_size:(train_size + valid_size)].reset_index(drop=True),\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "ins_dataset_test = Tomography(\n",
        "    df=noshuf_df[(train_size + valid_size):].reset_index(drop=True),\n",
        "    transform=data_transform,\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_train,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory = True\n",
        "\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_valid,\n",
        "    batch_size=batch_size, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory = True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_test,\n",
        "    batch_size=batch_size, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnOTAdZuLe2T"
      },
      "source": [
        "### 2 Layer w PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFmrpZzj6y9y"
      },
      "source": [
        "# Convolutional neural network\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes=len(names)):\n",
        "        super(ConvNet, self).__init__()\n",
        "  \n",
        "        # Add network layers here\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels= 1,\n",
        "                      out_channels=16,\n",
        "                      kernel_size= (3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2))\n",
        "            #,nn.Dropout(p = 0.3))\n",
        "        #self.conv2 = nn.Sequential(\n",
        "            #nn.Conv2d(16, 24, 4),\n",
        "            #nn.ReLU(),\n",
        "            #nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #nn.Dropout(p = 0.3))\n",
        "        #self.fc1 = nn.Linear(216, 512) \n",
        "        #self.fc2 = nn.Linear(512, num_classes)\n",
        "        ## With just one Conv\n",
        "        self.fc1 = nn.Linear(1296, num_classes) \n",
        "        #self.fc1 = nn.Linear(216, num_classes) \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Complete the graph\n",
        "        out = self.conv1(x)\n",
        "        #out = self.conv2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        \n",
        "        out = self.fc1(out)\n",
        "        #out = self.fc2(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "model = ConvNet()\n",
        "layers = 2\n",
        "runtime = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLjiSMH9AC5u"
      },
      "source": [
        "# Initialization CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4znha15rAA-Q"
      },
      "source": [
        "#train_model_epochs(num_epochs)\n",
        "from torch import optim\n",
        "import timeit\n",
        "LR = 0.02\n",
        "MO = 0.9\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_gpu = ConvNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Stochastic gradient descent\n",
        "optimizer = optim.SGD(model_gpu.parameters(),\n",
        "                      lr=LR,\n",
        "                      momentum=MO) \n",
        "model_gpu = model_gpu.to(device)\n",
        "cpu_train_time = timeit.timeit(\n",
        "    \"train_cnn_epochs(num_epochs)\",\n",
        "    setup=f\"num_epochs={runtime}\",\n",
        "    number=1,\n",
        "    globals=globals(),\n",
        ")\n",
        "print(f\"learning rate = {LR}\")\n",
        "print(f\"momentum = {MO}\")\n",
        "test_stats(model_gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUNLeauKEdnV"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labeltot, predtot, _, _ = test_stats(model_gpu)\n",
        "cm = confusion_matrix(labeltot, predtot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WZncxECEdnh"
      },
      "source": [
        "plot_confusion_matrix(cm, names)\n",
        "print(\">>> \\n\"\n",
        "              \"Test Accuracy: \\n\"\n",
        "              f\"{(100*sum(cm.diagonal())/sum(sum(cm[:,:]))):.2f}% \\n\"\n",
        "              )\n",
        "test_stats(model_gpu)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWENBDS4muXG"
      },
      "source": [
        "20/180 * () + 60/180 *() + 10/180 *()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5qpUN9g-aer"
      },
      "source": [
        "# Decision tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYycIdrOgQuq"
      },
      "source": [
        "label_set = 1\n",
        "grp_set = 5 # 6, 7\n",
        "lbl = ('Primary_Labels','Secondary_Labels')\n",
        "\n",
        "grp_list = ('Vectors_Single40th',  'Vectors_AV10', 'Vectors_AV20', 'Vectors_AV30',\n",
        "            'Vectors_AV40', 'Vectors_STD', 'Vectors_ACF1', 'Vectors_ACF5')\n",
        "\n",
        "directory = f\"/content/gdrive/My Drive/Tomography_Images/{lbl[label_set]}/{grp_list[grp_set]}/\"\n",
        "\n",
        "confusion_matrix_title = ['single image', '10 frames average', '20 frames average',\n",
        "                          '30 frames average', '40 frames average',\n",
        "                          '40 frames standard deviation', 'Auto-correlation: Lag 1', 'Auto-correlation: Lag 5']\n",
        "\n",
        "if label_set == 0:\n",
        "  label, names, name = 'Primary Labels', [\"Bubble_flow\",\"Plug_flow\",\"Stratified_flow\",\"Slug_flow\",\"Annular_flow\"], [\"Bubble\",\"Plug\",\"Stratified\",\"Slug\",\"Annular\"]\n",
        "elif label_set == 1:\n",
        "  label, names, name = 'Secondary Labels', [\"Bubble_flow\",\"Plug_flow\",\"Stratified_flow\",\"Slug_flow\",\"Transient_flow\"], [\"Bubble\",\"Plug\",\"Stratified\",\"Slug\",\"Transient\"]\n",
        "\n",
        "all_experiments = [\"194\",\"196\",\"198\",\"200\",\"202\", \"204\", \"206\", \"208\", \"209\",\"210\",\n",
        "                   \"212\",\"214\",\"216\",\"218\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\n",
        "                   \"227\",\"228\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sLdQIKpiHsn"
      },
      "source": [
        "# Benign images we will assign class 0, and malignant as 1\n",
        "paths, classes = get_data(directory, names)\n",
        "SVM_data = {\n",
        "    'path': paths,\n",
        "    'class': classes\n",
        "    }\n",
        "SVM_df = pd.DataFrame(SVM_data, columns=['path', 'class'])\n",
        "SVM_df = SVM_df.sample(frac=1).reset_index(drop=True) # Shuffles the data\n",
        "SVM_df['path'][1]\n",
        "# Code for both images\n",
        "lst = []\n",
        "for i in tqdm(range(len(SVM_df)), desc='Loading data...'):\n",
        "  test = pd.read_csv(SVM_df['path'][i])\n",
        "\n",
        "  if grp_list[grp_set] in ['Vectors_ACF1','Vectors_ACF5']:\n",
        "    t1 = t1.T.reset_index(drop=True).T.fillna(0)\n",
        "    t1.insert(316, 'class', SVM_df['class'][i])\n",
        "    lst.append(t1)\n",
        "\n",
        "  if grp_list[grp_set] not in  ['Vectors_ACF1' ,'Vectors_ACF5']:\n",
        "    t1 = test.iloc[:, 0:316]   #  Taking first images\n",
        "    t2 = test.iloc[:,316:634]  #  and second images\n",
        "    t1 = t1.T.reset_index(drop=True).T\n",
        "    t2 = t2.T.reset_index(drop=True).T\n",
        "    t1.insert(316, 'class', SVM_df['class'][i])   #  Inserting Class for both imgs\n",
        "    t2.insert(316, 'class', SVM_df['class'][i])\n",
        "    lst.append(t1)\n",
        "    lst.append(t2)\n",
        "\n",
        "tot = pd.concat(lst).to_numpy() # \n",
        "tot # (instances x parameters)\n",
        "# Code for first images only\n",
        "lst = []\n",
        "for i in range(len(SVM_df)):\n",
        "  test = pd.read_csv(SVM_df['path'][i])\n",
        "  test.insert(316, 'class', SVM_df['class'][i])\n",
        "  test2 = test.iloc[:, 0:317]\n",
        "  lst.append(test2)\n",
        "tot = pd.concat(lst).to_numpy()\n",
        "tot = np.nan_to_num(tot,nan=0.0)\n",
        "labels = tot[:,-1]\n",
        "labels = [int(class_tag) for class_tag in labels]\n",
        "images = tot[:,0:-1]\n",
        "unique(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P52Fv_BedXO"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(images,labels)\n",
        "y_train = [int(target) for target in y_train]\n",
        "y_test = [int(target) for target in y_test]\n",
        "\n",
        "print(f'Training data and target sizes: \\n{len(X_train)}, {len(y_train)}')\n",
        "print(f'Test data and target sizes: \\n{len(X_test)}, {len(y_test)}')\n",
        "X_test.shape\n",
        "print('\\n')\n",
        "X_test.shape\n",
        "print('\\n')\n",
        "X_train\n",
        "print('\\n')\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XcFB-7UHtMM"
      },
      "source": [
        "## Data augmentation: class enrichment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvyrfVYPUeHe"
      },
      "source": [
        "df = pd.DataFrame(zip(X_train,y_train)).rename(columns={0: \"path\"}).rename(columns={1: \"class\"})\n",
        "x = unique(df['class'])\n",
        "size = []\n",
        "extend, boundary = {}, {}\n",
        "for i in x:\n",
        "  porp = listcheck(df['class'],i)\n",
        "  if i == 0:\n",
        "    lst0 = df.sort_values(\"class\").iloc[0:porp]\n",
        "    l0 = len(lst0)\n",
        "  if i == 1:\n",
        "    lst1 = df.sort_values(\"class\").iloc[l0:l0+porp]\n",
        "    l1 = len(lst1)\n",
        "  if i == 2:\n",
        "    lst2 = df.sort_values(\"class\").iloc[l0+l1:l0+l1+porp]\n",
        "    l2 = len(lst2)\n",
        "  if i == 3:\n",
        "    lst3 = df.sort_values(\"class\").iloc[l0+l1+l2:l0+l1+l2+porp]\n",
        "    l3 = len(lst3)\n",
        "  if i == 4:\n",
        "    lst4 = df.sort_values(\"class\").iloc[l0+l1+l2+l3:l0+l1+l2+l3+porp]\n",
        "  print(f\"class {i} represents {listcheck(df['class'],i)} of the training elements\")\n",
        "  size.append(listcheck(df['class'],i))\n",
        "\n",
        "for i in x:\n",
        "  diff = max(size) - listcheck(df['class'],i)\n",
        "  extend[i] = diff\n",
        "  print(f\"{diff} difference for {i}\")\n",
        "  for j in range(diff):\n",
        "    pass\n",
        "\n",
        "tba0 = lst0.sample(extend[0], replace = True)\n",
        "tba1 = lst1.sample(extend[1], replace = True)\n",
        "tba2 = lst2.sample(extend[2], replace = True)\n",
        "tba3 = lst3.sample(extend[3], replace = True)\n",
        "tba4 = lst4.sample(extend[4], replace = True)\n",
        "tba1.iloc[1][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUy7wevPIAzd"
      },
      "source": [
        "imag0 = []\n",
        "for i in tqdm(range(len(tba0)), desc='Adding Gaussian noise....'):\n",
        "  test = tba0.iloc[i][0]\n",
        "  for j in range(316):\n",
        "    test[j] += abs(test[j] + np.random.normal(0,0.03,1))\n",
        "  imag0.append(test)\n",
        "imag1 = []\n",
        "for i in tqdm(range(len(tba1)), desc='Adding Gaussian noise....'):\n",
        "  test = tba1.iloc[i][0]\n",
        "  for j in range(316):\n",
        "    test[j] += abs(test[j] + np.random.normal(0,0.03,1))\n",
        "  imag1.append(test)\n",
        "imag2 = []\n",
        "for i in tqdm(range(len(tba2)), desc='Adding Gaussian noise....'):\n",
        "  test = tba2.iloc[i][0]\n",
        "  for j in range(316):\n",
        "    test[j] += abs(test[j] + np.random.normal(0,0.03,1))\n",
        "  imag2.append(test)\n",
        "\n",
        "imag3 = []\n",
        "for i in tqdm(range(len(tba3)), desc='Adding Gaussian noise....'):\n",
        "  test = tba3.iloc[i][0]\n",
        "  for j in range(316):\n",
        "    test[j] += abs(test[j] + np.random.normal(0,0.03,1))\n",
        "  imag3.append(test)\n",
        "\n",
        "imag4 = []\n",
        "for i in tqdm(range(len(tba4)), desc='Adding Gaussian noise....'):\n",
        "  test = tba4.iloc[i][0]\n",
        "  for j in range(316):\n",
        "    test[j] += abs(test[j] + np.random.normal(0,0.03,1))\n",
        "  imag4.append(test)\n",
        "zeros = [0]*len(imag0)\n",
        "ones = [1]*len(imag1)\n",
        "twos = [2]*len(imag2)\n",
        "threes = [3]*len(imag3)\n",
        "fours = [4]*len(imag4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIQqDD6qYm7v"
      },
      "source": [
        "x = df['path']\n",
        "y = df['class']\n",
        "zip0 = list(zip(imag0,zeros))\n",
        "class_zero = pd.DataFrame(zip0, columns=['path','class'])\n",
        "zip1 = list(zip(imag1,ones))\n",
        "class_one = pd.DataFrame(zip1, columns=['path','class'])\n",
        "zip2 = list(zip(imag2,twos))\n",
        "class_two = pd.DataFrame(zip2, columns=['path','class'])\n",
        "zip3 = list(zip(imag3,threes))\n",
        "class_three = pd.DataFrame(zip3, columns=['path','class'])\n",
        "zip4 = list(zip(imag4,fours))\n",
        "class_four = pd.DataFrame(zip4, columns=['path','class'])\n",
        "zip10 = list(zip(x,y))\n",
        "original_train = pd.DataFrame(zip10, columns=['path','class'])\n",
        "\n",
        "balanced_train = pd.concat([class_zero, class_one, class_two, class_three, class_four, original_train],\n",
        "                           ignore_index = True)\n",
        "\n",
        "freq_train = [listcheck(balanced_train['class'], 0),\n",
        "              listcheck(balanced_train['class'], 1),\n",
        "              listcheck(balanced_train['class'], 2),\n",
        "              listcheck(balanced_train['class'], 3),\n",
        "              listcheck(balanced_train['class'], 4)]\n",
        "\n",
        "classestrain = np.arange(5, dtype=int)\n",
        "plt.bar(classestrain, freq_train)\n",
        "plt.title('Secondary label distribution')\n",
        "plt.xlabel('Flow Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(classestrain, name)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsDwOA6Qt7M2"
      },
      "source": [
        "# **Balanced vs unbalanced training routine selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfQLv7OHYjCT"
      },
      "source": [
        "from numpy import asarray\n",
        "###########################################\n",
        "balanced_training = True # <--- alter this\n",
        "###########################################\n",
        "# training data for balanced set\n",
        "y_train_B = asarray(balanced_train['class'].values.tolist())\n",
        "X_train_B = asarray(balanced_train['path'].values.tolist())\n",
        "# want int targets\n",
        "y_train = [int(target) for target in y_train]\n",
        "\n",
        "if balanced_training == True:\n",
        "  X_train = X_train_B\n",
        "  y_train = y_train_B\n",
        "  print('Using balanced training set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94y3JqcZa_J"
      },
      "source": [
        "## For PCA Data Use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmeuwRm2ttep"
      },
      "source": [
        "img_norm = normalize(images)\n",
        "pca = PCA(.999)\n",
        "lower_dimension_data = pca.fit_transform(img_norm)\n",
        "approx = pca.inverse_transform(lower_dimension_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRphtkIDyF9I"
      },
      "source": [
        "v = pca.explained_variance_ratio_.cumsum()\n",
        "#plt.plot(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSoOTCZtaXOw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(approx,labels)\n",
        "print(f'Training data and target sizes: \\n{len(X_train)}, {len(y_train)}')\n",
        "print(f'Test data and target sizes: \\n{len(X_test)}, {len(y_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnhdoMSn-dob"
      },
      "source": [
        "#  Decision Tree: Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO_vdNMvbA7R"
      },
      "source": [
        "dec_tree = DecisionTreeClassifier()\n",
        "n_leaf = np.linspace(50, 150, 50).astype(int) # 52, ..., 148, 150\n",
        "# hyperparameter grid-search space\n",
        "params = {'max_leaf_nodes': n_leaf,\n",
        "          'min_samples_split': [2, 3, 4],\n",
        "          'criterion': ['gini', 'entropy']}\n",
        "\n",
        "classifier = GridSearchCV(estimator=dec_tree,\n",
        "                          param_grid=params,\n",
        "                          n_jobs=-1,\n",
        "                          verbose=10,\n",
        "                          cv=5)\n",
        "start = time()\n",
        "classifier.fit(X_train,\n",
        "               y_train)\n",
        "\n",
        "best_params = classifier.best_params_\n",
        "print(f\"GridSearchCV took {(time() - start):.2f} seconds\")\n",
        "best = DecisionTreeClassifier(**best_params)\n",
        "best.fit(X_train,\n",
        "         y_train)\n",
        "y_pred = best.predict(X_test)\n",
        "results = confusion_matrix(y_test,\n",
        "                           y_pred)\n",
        "\n",
        "plot_confusion_matrix(results,\n",
        "                      [m.replace('_', ' ') for m in names],\n",
        "                      title =f'Grid search decision tree confusion matrix:\\n{confusion_matrix_title[grp_set]}',\n",
        "                      cmap=plt.cm.Blues)\n",
        "\n",
        "print(classification_report(y_test,\n",
        "                            y_pred,\n",
        "                            target_names=[m.replace('_', ' ') for m in names]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P83sR28Uedqj"
      },
      "source": [
        "# Decision tree: Random Search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OFnmcbG8gwP"
      },
      "source": [
        "n_iter = 20 # number of randomized samples of the hyperparameter space -----> quick \n",
        "classifier = RandomizedSearchCV(dec_tree,\n",
        "                                param_distributions=params,\n",
        "                                n_iter=n_iter,\n",
        "                                cv=5,\n",
        "                                n_jobs=-1,\n",
        "                                verbose=10)\n",
        "start = time()\n",
        "classifier.fit(X_train,\n",
        "               y_train)\n",
        "\n",
        "print(f\"RandomizedSearchCV took {(time() - start):.2f} seconds\")\n",
        "best_params = classifier.best_params_\n",
        "best = DecisionTreeClassifier(**best_params)\n",
        "\n",
        "best.fit(X_train,\n",
        "         y_train)\n",
        "\n",
        "y_pred = best.predict(X_test)\n",
        "results = confusion_matrix(y_test,\n",
        "                          y_pred)\n",
        "\n",
        "plot_confusion_matrix(results,\n",
        "                      [m.replace('_', ' ') for m in names],\n",
        "                      title =f'Randomized search decision tree confusion matrix:\\n{confusion_matrix_title[grp_set]}',\n",
        "                      cmap=plt.cm.Blues)\n",
        "\n",
        "print(classification_report(y_test,\n",
        "                            y_pred,\n",
        "                            target_names=[m.replace('_', ' ') for m in names]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJEkE-YAjhCC"
      },
      "source": [
        "# Decision tree: Bayes optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-btGhe4jhCD"
      },
      "source": [
        "dec_tree = DecisionTreeClassifier()\n",
        "n_leaf = np.linspace(50, 150, 50).astype(int) # 52, ..., 148, 150\n",
        "n_iter = 20\n",
        "# hyperparameter grid-search space\n",
        "params = {'max_leaf_nodes': n_leaf,\n",
        "          'min_samples_split': [2, 3, 4],\n",
        "          'criterion': ['gini', 'entropy']}\n",
        "classifier = BayesSearchCV(dec_tree,\n",
        "                           search_spaces=params,\n",
        "                           n_iter=n_iter,\n",
        "                           cv=5,\n",
        "                           verbose=1)\n",
        "start = time()\n",
        "X_train = X_train.astype(float)\n",
        "classifier.fit(X_train,\n",
        "               y_train)\n",
        "\n",
        "print(f\"BayesSearchCV took {(time() - start):.2f} seconds\")\n",
        "\n",
        "best_params = classifier.best_params_\n",
        "best = DecisionTreeClassifier(**best_params)\n",
        "\n",
        "best.fit(X_train,\n",
        "         y_train)\n",
        "\n",
        "y_pred = best.predict(X_test)\n",
        "results = confusion_matrix(y_test,\n",
        "                          y_pred)\n",
        "\n",
        "plot_confusion_matrix(results,\n",
        "                      [m.replace('_', ' ') for m in names],\n",
        "                      title =f'Bayesian search decision tree confusion matrix:\\n{confusion_matrix_title[grp_set]}',\n",
        "                      cmap=plt.cm.Blues)\n",
        "\n",
        "print(classification_report(y_test,\n",
        "                            y_pred,\n",
        "                            target_names=[m.replace('_', ' ') for m in names]))\n",
        "\n",
        "# plt.figure(figsize=(150,50))\n",
        "# plot_tree(best,\n",
        "#          class_names=[m.replace('_', ' ') for m in names],\n",
        "#          filled = True,\n",
        "#          fontsize=18,\n",
        "#          rounded =True)\n",
        "\n",
        "# ETR_var_heatmap = np.reshape(best.feature_importances_, (20,20))\n",
        "# plt.imshow(ETR_var_heatmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f247Q2Y3z79T"
      },
      "source": [
        "# Random Forest: Grid-search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VJ0lwNVB8so"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "# Hyperparameter space\n",
        "params = {'max_leaf_nodes': np.linspace(20, 150, 10).astype(int),\n",
        "          'criterion': ['gini', 'entropy'],\n",
        "          'n_estimators': np.linspace(50, 150, 10).astype(int) }\n",
        "n_iter = 20 # number of randomized samples \n",
        "classifier = GridSearchCV(estimator=rf,\n",
        "                          param_grid=params,\n",
        "                          n_jobs=5,\n",
        "                          verbose = 10)\n",
        "start = time()\n",
        "classifier.fit(X_train,\n",
        "               y_train)\n",
        "print(f\"GridSearchCV took {(time() - start):.2f} seconds\" )\n",
        "\n",
        "best_params = classifier.best_params_\n",
        "best = RandomForestClassifier(**best_params)\n",
        "best.fit(X_train,\n",
        "         y_train)\n",
        "y_pred = best.predict(X_test)\n",
        "results = confusion_matrix(y_test,\n",
        "                           y_pred)\n",
        "\n",
        "plot_confusion_matrix(results,\n",
        "                      [m.replace('_', ' ') for m in names],\n",
        "                      title =f'Grid search random forest confusion matrix:\\n{confusion_matrix_title[grp_set]}',\n",
        "                      cmap=plt.cm.Blues)\n",
        "\n",
        "print(classification_report(y_test,\n",
        "                            y_pred,\n",
        "                            target_names=[m.replace('_', ' ') for m in names]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVQGP6gwG9vM"
      },
      "source": [
        "# Random forest : Randomized search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88kzQ_oxB1C3"
      },
      "source": [
        "n_iter = 20 # number of randomized hyperparameter space sampling \n",
        "classifier = RandomizedSearchCV(rf,\n",
        "                                param_distributions=params,\n",
        "                                n_iter=n_iter)\n",
        "start = time()\n",
        "classifier.fit(X_train,\n",
        "               y_train)\n",
        "\n",
        "print(f\"RandomizedSearchCV took {(time() - start):.2f} seconds\" )\n",
        "\n",
        "best_params = classifier.best_params_\n",
        "best = RandomForestClassifier(**best_params)\n",
        "best.fit(X_train,\n",
        "         y_train)\n",
        "\n",
        "y_pred = best.predict(X_test)\n",
        "results = confusion_matrix(y_test,\n",
        "                           y_pred)\n",
        "\n",
        "plot_confusion_matrix(results,\n",
        "                      [m.replace('_', ' ') for m in names],\n",
        "                      title =f'Randomized search random forest confusion matrix:\\n{confusion_matrix_title[grp_set]}',\n",
        "                      cmap=plt.cm.Blues)\n",
        "\n",
        "print(classification_report(y_test,\n",
        "                      y_pred,\n",
        "                      target_names=[m.replace('_', ' ') for m in names]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00yjLDVQfZzP"
      },
      "source": [
        "# Random forest: Bayes Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amuYXzKYoBC2"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "# Hyperparameter space\n",
        "params = {'max_leaf_nodes': np.linspace(20, 150, 10).astype(int),\n",
        "          'criterion': ['gini', 'entropy'],\n",
        "          'n_estimators': np.linspace(50, 150, 10).astype(int) }\n",
        "n_iter = 20 # number of randomized samples \n",
        "classifier = BayesSearchCV(rf,\n",
        "                           search_spaces=params,\n",
        "                           n_iter=n_iter,\n",
        "                           cv=5,\n",
        "                           verbose=1)\n",
        "start = time()\n",
        "classifier.fit(X_train,\n",
        "               y_train)\n",
        "print(f\"BayesSearchCV took {(time() - start):.2f} seconds\")\n",
        "\n",
        "best_params = classifier.best_params_\n",
        "best = RandomForestClassifier(**best_params)\n",
        "best.fit(X_train,\n",
        "         y_train)\n",
        "y_pred = best.predict(X_test)\n",
        "results = confusion_matrix(y_test,\n",
        "                          y_pred)\n",
        "plot_confusion_matrix(results,\n",
        "                      [m.replace('_', ' ') for m in names],\n",
        "                      title =f'Bayesian search random forest confusion matrix:\\n{confusion_matrix_title[grp_set]} balanced training set',\n",
        "                      cmap=plt.cm.Blues)\n",
        "\n",
        "print(classification_report(y_test,\n",
        "                      y_pred,\n",
        "                      target_names=[m.replace('_', ' ') for m in names]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD_GJvb7CRcn"
      },
      "source": [
        "rf = classifier.best_estimator_\n",
        "classifier.optimizer_results_[0]\n",
        "_ = plot_objective(classifier.optimizer_results_[0])\n",
        "plt.show()\n",
        "rf\n",
        "print(f\"val. score: {rf.best_score_}\" )\n",
        "print(f\"test score: {rf.score(X_test, y_test)}\")\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_cm = metrics.confusion_matrix(y_test, rf_pred)\n",
        "plot_confusion_matrix(rf_cm, names)\n",
        "\n",
        "print(\">>> \\n\"\n",
        "              \"Test Accuracy: \\n\"\n",
        "              f\"{(100*sum(rf_cm.diagonal())/sum(sum(rf_cm[:,:]))):.2f}% \\n\"\n",
        "              )\n",
        "#rf.estimators_[0]\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_train.shape[1]):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "importances = (importances/max(importances))\n",
        "error_rate = []\n",
        "for i in range(50, 100):\n",
        "    rf.set_params(n_estimators=i)\n",
        "    rf.fit(X_train, y_train)\n",
        "    oob_error = 1 - rf.oob_score_\n",
        "    error_rate.append((i, oob_error))\n",
        "error_rate[0]\n",
        "x,y = [],[]\n",
        "for i in range(len(error_rate)):\n",
        "  x.append(error_rate[i][0])\n",
        "  y.append(error_rate[i][1])\n",
        "print(y)\n",
        "plt.plot(x,y, label = 'RandomForest')\n",
        "plt.xlim(50, 60)\n",
        "plt.xlabel(\"n_estimators\")\n",
        "plt.ylabel(\"OOB error rate\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsDCQk5fJNva"
      },
      "source": [
        "# **Additional models: not reported**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip0Sk1jwXlpa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import OrderedDict\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "RANDOM_STATE = 123\n",
        "\n",
        "# Generate a binary classification dataset.\n",
        "X, y = make_classification(n_samples=500, n_features=25,\n",
        "                           n_clusters_per_class=1, n_informative=15,\n",
        "                           random_state=RANDOM_STATE)\n",
        "\n",
        "\n",
        "ensemble_clfs = [\n",
        "    (\"RandomForestClassifier,ax_features m='sqrt'\",\n",
        "        RandomForestClassifier(warm_start=True, oob_score=True,\n",
        "                               max_features=\"sqrt\",\n",
        "                               random_state=RANDOM_STATE)),\n",
        "    (\"RandomForestClassifier, max_features='log2'\",\n",
        "        RandomForestClassifier(warm_start=True, max_features='log2',\n",
        "                               oob_score=True,\n",
        "                               random_state=RANDOM_STATE)),\n",
        "    (\"RandomForestClassifier, max_features=None\",\n",
        "        RandomForestClassifier(warm_start=True, max_features=None,\n",
        "                               oob_score=True,\n",
        "                               random_state=RANDOM_STATE))\n",
        "]\n",
        "\n",
        "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
        "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
        "\n",
        "# Range of `n_estimators` values to explore.\n",
        "min_estimators = 15\n",
        "max_estimators = 175\n",
        "\n",
        "for label, clf in ensemble_clfs:\n",
        "    for i in range(min_estimators, max_estimators + 1):\n",
        "        clf.set_params(n_estimators=i)\n",
        "        clf.fit(X, y)\n",
        "\n",
        "        # Record the OOB error for each `n_estimators=i` setting.\n",
        "        oob_error = 1 - clf.oob_score_\n",
        "        error_rate[label].append((i, oob_error))\n",
        "\n",
        "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
        "for label, clf_err in error_rate.items():\n",
        "    xs, ys = zip(*clf_err)\n",
        "    print(ys)\n",
        "    #plt.plot(xs, ys, label=label)\n",
        "\n",
        "plt.xlim(min_estimators, max_estimators)\n",
        "plt.xlabel(\"n_estimators\")\n",
        "plt.ylabel(\"OOB error rate\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU63Xm_7Sngo"
      },
      "source": [
        "for err in error_rate:\n",
        "    x, y = err\n",
        "    plt.plot(x, y)\n",
        "\n",
        "plt.xlim(min_estimators, max_estimators)\n",
        "plt.xlabel(\"n_estimators\")\n",
        "plt.ylabel(\"OOB error rate\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UceoJ46bglqC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import OrderedDict\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
        "error_rate = []\n",
        "\n",
        "# Range of `n_estimators` values to explore.\n",
        "min_estimators = 15\n",
        "max_estimators = 175\n",
        "\n",
        "for i in range(min_estimators, max_estimators + 1):\n",
        "    rf.set_params(n_estimators=i)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Record the OOB error for each `n_estimators=i` setting.\n",
        "    oob_error = 1 - rf.oob_score_\n",
        "    error_rate.append((i, oob_error))\n",
        "\n",
        "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
        "for label, clf_err in error_rate.items():\n",
        "    xs, ys = zip(*clf_err)\n",
        "    plt.plot(xs, ys, label=label)\n",
        "\n",
        "plt.xlim(min_estimators, max_estimators)\n",
        "plt.xlabel(\"n_estimators\")\n",
        "plt.ylabel(\"OOB error rate\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmHupEaKK_4S"
      },
      "source": [
        "importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8WeomCEvmKE"
      },
      "source": [
        "# Saving NumPy array as a csv file\n",
        "array_rain_fall = np.loadtxt(fname=\"rain-fall.csv\",\n",
        "                             delimiter=\",\")\n",
        "np.savetxt(fname=\"saved-rain-fall-row-col-names.csv\",\n",
        "           delimiter=\",\",\n",
        "           X=array_rain_fall)\n",
        "\n",
        "# Check generated csv file after loading it\n",
        "\n",
        "array_rain_fall_csv_saved = np.loadtxt(\n",
        "    fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\"\n",
        ")\n",
        "\n",
        "print(\"NumPy array: \\n\", array_rain_fall_csv_saved)\n",
        "print(\"Shape: \", array_rain_fall_csv_saved.shape)\n",
        "print(\"Data Type: \", array_rain_fall_csv_saved.dtype.name)\n",
        "\n",
        "# We can change our delimeter and save file in tsv or other text format\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmXW2eVxvlbY"
      },
      "source": [
        "# Saving NumPy array as a csv file\n",
        "array_rain_fall = np.loadtxt(fname=\"rain-fall.csv\", delimiter=\",\")\n",
        "np.savetxt(fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\", X=array_rain_fall)\n",
        "\n",
        "# Check generated csv file after loading it\n",
        "array_rain_fall_csv_saved = np.loadtxt(\n",
        "    fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\"\n",
        ")\n",
        "print(\"NumPy array: \\n\", array_rain_fall_csv_saved)\n",
        "print(\"Shape: \", array_rain_fall_csv_saved.shape)\n",
        "print(\"Data Type: \", array_rain_fall_csv_saved.dtype.name)\n",
        "# We can change our delimeter and save file in tsv or other text format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVZyX7HItnhm"
      },
      "source": [
        "# Saving NumPy array as a csv file\n",
        "array_rain_fall = np.loadtxt(fname=\"rain-fall.csv\", delimiter=\",\")\n",
        "np.savetxt(fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\", X=array_rain_fall)\n",
        "\n",
        "# Check generated csv file after loading it\n",
        "\n",
        "array_rain_fall_csv_saved = np.loadtxt(\n",
        "    fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\"\n",
        ")\n",
        "\n",
        "print(\"NumPy array: \\n\", array_rain_fall_csv_saved)\n",
        "print(\"Shape: \", array_rain_fall_csv_saved.shape)\n",
        "print(\"Data Type: \", array_rain_fall_csv_saved.dtype.name)\n",
        "\n",
        "# We can change our delimeter and save file in tsv or other text format\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smu_RzHj0eSG"
      },
      "source": [
        "#tree.plot_tree(rf.estimators_[0], class_names=class_names, filled = True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtECxCMf7eED"
      },
      "source": [
        "## K-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN0D8nEF7ilm"
      },
      "source": [
        "rf_kfold = cross_validate(rf, X_train, y_train, cv=5)\n",
        "print(rf_kfold['test_score'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh8TZ8zCeQN5"
      },
      "source": [
        "# Adaptive Boosting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfC1yZijeNhW"
      },
      "source": [
        "ab = AdaBoostClassifier()\n",
        "params = { 'n_estimators':np.linspace(20, 150, 16).astype(int) ,\n",
        "          'learning_rate': np.linspace(0, 2, 200),\n",
        "          'algorithm':['SAMME', 'SAMME.R']}\n",
        "ab.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReNgmtKjWpXd"
      },
      "source": [
        "## Grid Search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79UPySKQWpXi"
      },
      "source": [
        "classifier = GridSearchCV(estimator=ab, param_grid=params, n_jobs=5, verbose = 10)\n",
        "start = time()\n",
        "classifier.fit(X_train, y_train)\n",
        "print(\"GridSearchCV took %.2f seconds\" % (time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxxIbp2HWpXn"
      },
      "source": [
        "## Random Search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iYUHO_tWpXo"
      },
      "source": [
        "classifier = RandomizedSearchCV(ab, param_distributions=params, n_iter=n_iter)\n",
        "start = time()\n",
        "classifier.fit(X_train, y_train)\n",
        "print(\"RandomizedSearchCV took %.2f seconds\" % (time() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2ErQ78WpXw"
      },
      "source": [
        "## Bayes Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3qu8AlPWpXy"
      },
      "source": [
        "classifier = BayesSearchCV(ab,search_spaces=params,n_iter=n_iter,cv=3)\n",
        "start = time()\n",
        "classifier.fit(X_train, y_train)\n",
        "print(\"BayesSearchCV took %.2f seconds\" % (time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4xrrIq8WpX1"
      },
      "source": [
        "ab = classifier.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmB2lNI0rrok"
      },
      "source": [
        "_ = plot_objective(classifier.optimizer_results_[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBoKVVZaftjX"
      },
      "source": [
        "rf_cm = metrics.confusion_matrix(y_test, rf_pred)\n",
        "plot_confusion_matrix(rf_cm, names)\n",
        "\n",
        "print(\">>> \\n\"\n",
        "              \"Test Accuracy: \\n\"\n",
        "              f\"{(100*sum(rf_cm.diagonal())/sum(sum(rf_cm[:,:]))):.2f}% \\n\"\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Qruftk2RNO"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vas3udZferiw"
      },
      "source": [
        "# Create a classifier: a support vector classifier\n",
        "svmm = svm.SVC()\n",
        "Cs = np.linspace(100, 300, 40)\n",
        "\n",
        "#folds = 5\n",
        "#skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 999)\n",
        "classifier = GridSearchCV(estimator=svmm, param_grid=dict(C=Cs), n_jobs=5, cv = 5,\n",
        "                          return_train_score = True, verbose=10)\n",
        "classifier.fit(X_train,y_train)\n",
        "#classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2EpOm4BVx91"
      },
      "source": [
        "classifier.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kKl5BfkUqO4"
      },
      "source": [
        "results = classifier.cv_results_\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "results_df['split0_test_score']\n",
        "results_df['param_C']\n",
        "\n",
        "#best_mean = \n",
        "#for i in range(len(results_df)):\n",
        "#  results_df['mean_test_score '].iloc[i]\n",
        "\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZkJTrLzQQr9"
      },
      "source": [
        "GridSearch_table_plot(classifier, 'C')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZTkQU7Zexbb"
      },
      "source": [
        "# Best Classifier for PCA'ed data\n",
        "classifier = svm.SVC(C = 158.333)\n",
        "classifier.fit(X_train,y_train)\n",
        "svm_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkNilXaVj82M"
      },
      "source": [
        "# Best Classifier for non-PCA'ed data\n",
        "classifier = svm.SVC(C = 166.666)\n",
        "classifier.fit(X_train,y_train)\n",
        "svm_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8Nx7P458ave"
      },
      "source": [
        "print(classification_report(y_test, svm_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltej4e2Klmm6"
      },
      "source": [
        "len(X_test[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrkMVr7ml9mI"
      },
      "source": [
        "img = Image.fromarray(X_test[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pp5Is72lbg2"
      },
      "source": [
        "svm_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zm-FL-llefc"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YbC0QXVeyy1"
      },
      "source": [
        "print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, svm_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny3m26FufGeP"
      },
      "source": [
        "svm_cm = metrics.confusion_matrix(y_test, svm_pred)\n",
        "plot_confusion_matrix(svm_cm, class_names)\n",
        "\n",
        "print(\">>> \\n\"\n",
        "              \"Test Accuracy: \\n\"\n",
        "              f\"{(100*sum(svm_cm.diagonal())/sum(sum(svm_cm[:,:]))):.2f}% \\n\"\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBMfRgYe-iQg"
      },
      "source": [
        "# KNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ngH-zifoh-s"
      },
      "source": [
        "np.linspace(5,35,7).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmibzZO3n-x6"
      },
      "source": [
        "leafs = np.linspace(5,35,7).astype(int)\n",
        "knn = KNeighborsClassifier()\n",
        "params = { #'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "          'leaf_size': leafs , \n",
        "          'metric':['manhattan'],\n",
        "                    #'minkowski','euclidean','manhattan'],\n",
        "                     'n_neighbors':[3,5,7,11,13],\n",
        "                     #'weights':['uniform','distance']\n",
        "           }\n",
        "classifier = GridSearchCV(estimator=knn, param_grid=params, n_jobs=5,\n",
        "                          verbose=10,return_train_score = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyPY3jtpFHO"
      },
      "source": [
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeUCDIdts6C_"
      },
      "source": [
        "max(results_df['mean_test_score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhK_XBcU3JCb"
      },
      "source": [
        "classifier.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfukOjwzuGD1"
      },
      "source": [
        "results = classifier.cv_results_\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEvxwWwu-kY0"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=3, metric = 'manhattan')\n",
        "knn.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoJuShHw6ycQ"
      },
      "source": [
        "knn_pred = knn.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JBWE7yX6_Io"
      },
      "source": [
        "knn_cm = metrics.confusion_matrix(y_test, knn_pred)\n",
        "plot_confusion_matrix(knn_cm, class_names)\n",
        "\n",
        "print(\">>> \\n\"\n",
        "              \"Test Accuracy: \\n\"\n",
        "              f\"{(100*sum(knn_cm.diagonal())/sum(sum(knn_cm[:,:]))):.2f}% \\n\"\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkuQ6-W-Adit"
      },
      "source": [
        "## K-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K4zP2HPAdDr"
      },
      "source": [
        "knn_kfold = cross_validate(knn, X_train, y_train, cv=5)\n",
        "knn_kfold['test_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJXcCOyvQ3my"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAtmgap6mz0L"
      },
      "source": [
        "dec_tree = DecisionTreeClassifier()\n",
        "params = {'solver': ['lbfgs'],\n",
        "          'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ],\n",
        "          'alpha': 10.0 ** -np.arange(1, 10),\n",
        "          'hidden_layer_sizes':np.linspace(5, 20,1), \n",
        "          'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
        "\n",
        "          \n",
        "classifier = GridSearchCV(estimator=dec_tree, param_grid=params, n_jobs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxMOqA_tQ2-n"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=500)\n",
        "nn.fit(X_train,y_train)\n",
        "\n",
        "nn_pred = nn.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INQHOKGgRWCK"
      },
      "source": [
        "nn_cm = metrics.confusion_matrix(y_test, nn_pred)\n",
        "plot_confusion_matrix(nn_cm, class_names)\n",
        "\n",
        "print(\">>> \\n\"\n",
        "              \"Test Accuracy: \\n\"\n",
        "              f\"{(100*sum(nn_cm.diagonal())/sum(sum(nn_cm[:,:]))):.2f}% \\n\"\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWRpdzlHAq0Q"
      },
      "source": [
        "## K-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih_eeKZjAz5u"
      },
      "source": [
        "nn_kfold = cross_validate(nn, X_train, y_train, cv=5)\n",
        "nn_kfold['test_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0bc01mJXC-K"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jliIaBDnXgj-"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "logreg_pred = logreg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8BudMvYX7W6"
      },
      "source": [
        "logreg_cm = metrics.confusion_matrix(y_test, logreg_pred)\n",
        "plot_confusion_matrix(logreg_cm, class_names)\n",
        "\n",
        "print(\">>> \\n\"\n",
        "              \"Test Accuracy: \\n\"\n",
        "              f\"{(100*sum(logreg_cm.diagonal())/sum(sum(logreg_cm[:,:]))):.2f}% \\n\"\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYjr3Wm0A48e"
      },
      "source": [
        "## K-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zEw_0bRA6xU"
      },
      "source": [
        "logreg_kfold = cross_validate(logreg, X_train, y_train, cv=5)\n",
        "logreg_kfold['test_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A8wbs3KZtdH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5jgAQBTZdA1"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPxsMXwnzYMQ"
      },
      "source": [
        ""
      ]
    }
  ]
}